<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />








  <meta property="fb:admins" content="100035055370515" />
  <meta property="fb:app_id" content="473188823418438" />



<meta name="description" content="我听说虫洞可以穿梭时空">
<meta property="og:type" content="website">
<meta property="og:title" content="在人间漂流">
<meta property="og:url" content="https://brooksj.com/index.html">
<meta property="og:site_name" content="在人间漂流">
<meta property="og:description" content="我听说虫洞可以穿梭时空">
<meta property="article:author" content="brooksj">
<meta property="article:tag" content="hiphop | rock | basketball | movie">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://brooksj.com/"/>





<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">

  <title>在人间漂流</title>
  
<script>
  window.fbAsyncInit = function() {
    FB.init({
      appId      : '473188823418438',
      xfbml      : true,
      version    : 'v2.10'
    });
  };

  (function(d, s, id){
     var js, fjs = d.getElementsByTagName(s)[0];
     if (d.getElementById(id)) {return;}
     js = d.createElement(s); js.id = id;
     js.src = "//connect.facebook.net/zh_Hans/sdk.js";
     fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script>









<meta name="generator" content="Hexo 4.2.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">


  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <!-- fork me on github -->
    <!-- a href="https://github.com/tracy-talent"><img width="149" height="149" src="https://github.blog/wp-content/uploads/2008/12/forkme_left_darkblue_121621.png?resize=149%2C149" class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a -->
    
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">在人间漂流</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">厚德 求真 励学 笃行</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <a href="https://github.com/tracy-talent" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub">
        <svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true">
            <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
            <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
            <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path>
        </svg>
    </a>
    <style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      


  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">

    <link itemprop="mainEntityOfPage" href="https://brooksj.com/2020/10/31/python%E6%AD%A3%E5%88%99%E5%8C%B9%E9%85%8Dmatch-search-findall-finditer%E5%8C%BA%E5%88%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="brooksj">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="在人间漂流">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/10/31/python%E6%AD%A3%E5%88%99%E5%8C%B9%E9%85%8Dmatch-search-findall-finditer%E5%8C%BA%E5%88%AB/" itemprop="url">python正则匹配match,search,findall,finditer区别</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-10-31T14:08:34+08:00">
                2020-10-31
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/language/" itemprop="url" rel="index">
                    <span itemprop="name">language</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/language/python/" itemprop="url" rel="index">
                    <span itemprop="name">python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/10/31/python%E6%AD%A3%E5%88%99%E5%8C%B9%E9%85%8Dmatch-search-findall-finditer%E5%8C%BA%E5%88%AB/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="https://brooksj.com/2020/10/31/python%E6%AD%A3%E5%88%99%E5%8C%B9%E9%85%8Dmatch-search-findall-finditer%E5%8C%BA%E5%88%AB/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          
          
             <span id="/2020/10/31/python%E6%AD%A3%E5%88%99%E5%8C%B9%E9%85%8Dmatch-search-findall-finditer%E5%8C%BA%E5%88%AB/" class="leancloud_visitors" data-flag-title="python正则匹配match,search,findall,finditer区别">
               <span class="post-meta-divider"><br/></span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          <span class="post-time">
	        &nbsp; | &nbsp;
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">字数统计:</span>
            <span class="post-count">283(字)</span>
            
          </span>
	  
          <span class="post-time">
	        &nbsp; | &nbsp;
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">阅读时长:</span>
            <span class="post-count">1(分)</span>
          </span>

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>python正则匹配常用到的函数主要主要有：</p>
<ol>
<li>fullmatch/match，返回re.Match对象，Match对象调用group(0)返回整个匹配，调用group(1)或者group(2)等依次返回括号内的匹配</li>
<li>search，返回re.Match对象</li>
<li>findall，返回列表包含括号内的匹配</li>
<li>finditer，返回re.Match迭代器</li>
</ol>
<p>示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">'333STR1666STR299'</span></span><br><span class="line">regex = <span class="string">r'[A-Z]+(\d)'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    print(re.match(regex, content)) <span class="comment">##content的开关不符合正则，所以结果为None。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">##只会找一个匹配，match[0]是regex所代表的整个字符串，match[1]是第一个()中的内容，match[2]是第二对()中的内容。</span></span><br><span class="line">    match = re.search(regex, content)</span><br><span class="line">    print(<span class="string">'\nre.search() return value: '</span> + str(type(match)))</span><br><span class="line">    print(match.group(<span class="number">0</span>), match.group(<span class="number">1</span>))  </span><br><span class="line"></span><br><span class="line">    result1 = re.findall(regex, content)</span><br><span class="line">    print(<span class="string">'\nre.findall() return value: '</span> + str(type(result1)))</span><br><span class="line">    print(result1)</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> result1:</span><br><span class="line">        print(m)</span><br><span class="line"></span><br><span class="line">    result2 = re.finditer(regex, content)</span><br><span class="line">    print(<span class="string">'\nre.finditer() return value: '</span> + str(type(result2)))</span><br><span class="line">    print(result2)</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> result2:</span><br><span class="line">        print(m.group(<span class="number">0</span>), m.group(<span class="number">1</span>))  <span class="comment">##字符串</span></span><br></pre></td></tr></table></figure>
<p>返回结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">None</span><br><span class="line"></span><br><span class="line">re.search() return value: &lt;class &#39;re.Match&#39;&gt;</span><br><span class="line">STR1 1</span><br><span class="line"></span><br><span class="line">re.findall() return value: &lt;class &#39;list&#39;&gt;</span><br><span class="line">[&#39;1&#39;, &#39;2&#39;]</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line"></span><br><span class="line">re.finditer() return value: &lt;class &#39;callable_iterator&#39;&gt;</span><br><span class="line">&lt;callable_iterator object at 0x7ffa374dfad0&gt;</span><br><span class="line">STR1 1</span><br><span class="line">STR2 2</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    
    
    <!-- 文章结束分割线 -->
    <div style="text-align:center;color: #ccc;font-size:14px;">---------------- The End ----------------</div>

    

    

    
    
    <div>
	    <p id="div-border-left-red">
	    <span>
        <b>作者: brooksjay</b><br/>
        <b>联系邮箱: jaypark@smail.nju.edu.cn</b><br/>
	    <b>本文地址: </b><a href="/index.html" title=""></a><br/>
	    <b>本文基于<a target="_blank" title="Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)" href="http://creativecommons.org/licenses/by-sa/4.0/"> 知识共享署名-相同方式共享 4.0 </a>国际许可协议发布</b><br/>
        <b>转载请注明出处, 谢谢！</b>
	    </span>
	    </p>
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      


  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">

    <link itemprop="mainEntityOfPage" href="https://brooksj.com/2020/01/19/ELMo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="brooksj">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="在人间漂流">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/19/ELMo/" itemprop="url">ELMo</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-19T23:35:03+08:00">
                2020-01-19
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/01/19/ELMo/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="https://brooksj.com/2020/01/19/ELMo/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          
          
             <span id="/2020/01/19/ELMo/" class="leancloud_visitors" data-flag-title="ELMo">
               <span class="post-meta-divider"><br/></span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          <span class="post-time">
	        &nbsp; | &nbsp;
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">字数统计:</span>
            <span class="post-count">752(字)</span>
            
          </span>
	  
          <span class="post-time">
	        &nbsp; | &nbsp;
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">阅读时长:</span>
            <span class="post-count">3(分)</span>
          </span>

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>ELMo(Embeddings from Language Models)是发表在NAACL2018的论文<a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="noopener">Deep contextualized word representations</a>提出来的模型，属于feature-based预训练语言模型，其后的<a href="https://arxiv.org/abs/1801.06146" target="_blank" rel="noopener">ULM-fit</a>与其模型基本一致，但在训练和迁移阶段使用了更多的分层适用的概念，属于fine-tuning预训练语言模型。</p>
<p>ELMo模型的简略模型结构图如下</p>
<div align="center">
    <img src="/images/NLP/elmo_5.png">
</div>

<p>主要分为两大模块：</p>
<ol>
<li>token representation</li>
<li>Multi-BiLSTM</li>
</ol>
<p>下面分别给出这两个模块的一些细节</p>
<h2 id="token-representation"><a href="#token-representation" class="headerlink" title="token representation"></a>token representation</h2><p>ELMo模型的token representation采用了character embedding，拥有更强大的表示能力，并且使用多个不同跨度的卷积核进行一维卷积，不同kernel卷积得到的不同表示拼接在一起，然后经过两层highway network得到最终的预处理编码表示。下图是一维卷积获取character embedding的具体操作示意图</p>
<div align="center">
    <img src="/images/NLP/elmo_2.png">
</div>

<p>多跨度多kernel的卷积拼接获取character embedding的具体操作示意图如下</p>
<div align="center">
    <img src="/images/NLP/elmo_3.png">
</div>

<p>将拼接得到的character embedding输入到两层highway network进行处理，highway network其实就是CV领域著名的ResNet中使用的残差单元的原型，highway network模型结构图如下</p>
<div align="center">
    <img src="/images/NLP/elmo_1.png">
</div>

<p>highway network的表达式描述为</p>
<script type="math/tex; mode=display">
y=H(x,W_H) \cdot T(x, W_T) + x \cdot C(x, W_C)</script><p>特别地，当$C=1-T$</p>
<script type="math/tex; mode=display">
y=H(x,W_H) \cdot T(x, W_T) + x \cdot (1 - T(x, W_H))</script><p>其中$H,T,C$都是非线性转换，$W_H,W_T,W_C$分别为它们对应的参数。</p>
<h2 id="Multi-BiLSTM"><a href="#Multi-BiLSTM" class="headerlink" title="Multi-BiLSTM"></a>Multi-BiLSTM</h2><p>下图是LSTM的模型结构试图以及每个control gate的计算式</p>
<div align="center">
    <img src="/images/NLP/elmo_4.png">
</div>

<p>BiLSTM则是使用一个正向一个逆向的LSTM处理sequence得到每个token的hidden state进行拼接作为这个token最终的输出。而Multi-BiLSTM顾名思义就是多层的双向LSTM，将当前这一层BiLSTM的正反向hidden state拼接输出作为紧邻的上一层的输入，最后一层的输出才是token最终的输出。</p>
<h2 id="downstream-application"><a href="#downstream-application" class="headerlink" title="downstream application"></a>downstream application</h2><p>ELMo使用feature-based的方式应用到下游任务，就是将模型输出表示加入(一般是拼接)到任务模型中的某一层中或多层中以增强任务模型对文本的表示能力。具体方式在这里我直接引用原文更好理解：</p>
<p>a L-layer biLM computes a set of 2L + 1 representations</p>
<script type="math/tex; mode=display">
\begin{aligned} R_{k} &=\left\{\mathbf{x}_{k}^{L M}, \overrightarrow{\mathbf{h}}_{k, j}^{L M}, \overleftarrow{\mathbf{h}}_{k, j}^{L M} | j=1, \ldots, L\right\} \\ &=\left\{\mathbf{h}_{k, j}^{L M} | j=0, \ldots, L\right\} \end{aligned}</script><p>$h<em>{k,0}^{LM}=[x</em>{k}^{LM},x_{k}^{LM}]$  &nbsp;for token representation</p>
<p>$h<em>{k,j}^{LM}=[\overrightarrow{\mathbf{h}}</em>{k, j}^{L M}, \overleftarrow{\mathbf{h}}_{k, j}^{L M}],j \gt 1$ &nbsp;for each biLSTM layer, compute a task specific weighting of all LM layers</p>
<script type="math/tex; mode=display">
\text { ELMo }_{k}^{\text {task }}=E\left(R_{k} ; \Theta^{\text {task }}\right)=\gamma^{\text {task }} \sum_{j=0}^{L} s_{j}^{\text {task }} \mathbf{h}_{k, j}^{L M}</script><p>$s_{j}^{\text {task}}$are softmax-normalized weights and the scalar parameter $\gamma^{\text {task }}$allows the task model to scale the entire ELMo vector.</p>
<p>下游任务可以根据优化的目标函数对ELMo表示的权重$s_{j}^{\text {task}},\gamma^{\text {task }}$进行学习得到合适的权重。</p>

          
        
      
    </div>
    
    
    
    
    <!-- 文章结束分割线 -->
    <div style="text-align:center;color: #ccc;font-size:14px;">---------------- The End ----------------</div>

    

    

    
    
    <div>
	    <p id="div-border-left-red">
	    <span>
        <b>作者: brooksjay</b><br/>
        <b>联系邮箱: jaypark@smail.nju.edu.cn</b><br/>
	    <b>本文地址: </b><a href="/index.html" title=""></a><br/>
	    <b>本文基于<a target="_blank" title="Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)" href="http://creativecommons.org/licenses/by-sa/4.0/"> 知识共享署名-相同方式共享 4.0 </a>国际许可协议发布</b><br/>
        <b>转载请注明出处, 谢谢！</b>
	    </span>
	    </p>
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      


  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">

    <link itemprop="mainEntityOfPage" href="https://brooksj.com/2019/12/06/Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="brooksj">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="在人间漂流">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/12/06/Transformer/" itemprop="url">Transformer</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-12-06T21:14:01+08:00">
                2019-12-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/12/06/Transformer/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="https://brooksj.com/2019/12/06/Transformer/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          
          
             <span id="/2019/12/06/Transformer/" class="leancloud_visitors" data-flag-title="Transformer">
               <span class="post-meta-divider"><br/></span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          <span class="post-time">
	        &nbsp; | &nbsp;
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">字数统计:</span>
            <span class="post-count">2.9k(字)</span>
            
          </span>
	  
          <span class="post-time">
	        &nbsp; | &nbsp;
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">阅读时长:</span>
            <span class="post-count">11(分)</span>
          </span>

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在transformer model出现之前，主流的sequence transduction model是基于循环或者卷积神经网络，表现最好的模型也是用attention mechanism连接基于循环神经网络的encoder和decoder.</p>
<p>Transformer model是一种摒弃了循环及卷积结构，仅仅依赖于注意力机制的简洁的神经网络模型。我们知道recurrent network是一种sequential model，不能很好地解决长距离依赖的问题(序列过长时，信息在序列模型中传递时容易一点点丢失)，并且阻碍了parallelism within train example.而<font color='red'>transform最引人瞩目的一点正是很好地解决了长距离依赖的问题，通过引入自注意力机制(self-attention)使得对依赖的建模与输入输出序列的距离无关，并且支持train exmaple内部的并行化。</font>注意力机制可以参考之前写的<a href="https://brooksj.com/2019/12/01/Standford-CS224n-Notes/">notes of cs224n lecture8</a>.</p>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>下图1是transformer的模型架构图</p>
<p><img src="/images/NLP/transformer_1.png" align="center"></p>
<center>图1. The Transformer - model architecture</center>



<p>左边是encoder，右边是decoder，各有6层，下面我将讲解个人觉得比较重要的几个点。</p>
<h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p><img src="/images/NLP/transformer_2.png" align="center"></p>
<center>图2. (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.
3.2.1</center>

<h4 id="Scaled-Dot-Producted-Attention"><a href="#Scaled-Dot-Producted-Attention" class="headerlink" title="Scaled Dot-Producted Attention"></a>Scaled Dot-Producted Attention</h4><p>如上图2左边所示即为dot-producted attention的一般形式，在transformer中，初始Q, K, V即为一个句子所有的subword编码构成的矩阵(seq_len, d_model)，其中d_model是subword的编码长度。Deocder的第二个Multi-Head Attention中的Q, K, V会有所不同，在讲解Multi-Head Attention是提到。下面是dot-producted attention的数学表达形式</p>
<script type="math/tex; mode=display">
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_{k}}})V</script><p>这里除以<script type="math/tex">\sqrt{d_k}</script>是因为在与<a href="https://brooksj.com/2019/12/01/Standford-CS224n-Notes/">additive attention</a>(之前写的文章有提到)对比时发现<script type="math/tex">d_k</script>较小时，这两种注意力机制表现相似，但是当<script type="math/tex">d_k</script>较大时dot-producted attention不如additive attention。最后分析原因发现<script type="math/tex">d_k</script>较大时<script type="math/tex">QK^T</script>点乘结果矩阵元素在数量级很大，使得softmax函数求导的梯度非常小。下面是原论文的一个解释</p>
<blockquote>
<p>To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, <script type="math/tex">q \cdot d=\sum_{i=1}^{d_k}{q_{i}k_{i}}</script>, has mean 0 and variance dk.</p>
</blockquote>
<h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><p>如上图2右边所示即为Multi-Head Attention的结构图。它的中心思想就是将原来的<script type="math/tex">(Q, K, V)</script>转换成num_heads个shape为(seq_len, d_model/num_heads)的<script type="math/tex">(Q_i, K_i, V_i)</script>，然后再为每个head执行dot-producted attention的操作，最后再将所有head的输出在最后一个维度上进行拼接得到与对原始输入<script type="math/tex">(Q, K, V)</script>执行单个dot-producted attention操作后shape一致的结果。下面是Multi-Attention的数学表达形式</p>
<script type="math/tex; mode=display">
\begin{aligned} \text { MultiHead }(Q, K, V) &=\text { Concat }\left(\text { head }_{1}, \ldots, \text { head }_{\mathrm{h}}\right) W^{O} \\ \text { where head }_{\mathrm{i}} &=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \end{aligned}</script><p>其中，<script type="math/tex">W^{Q}_{i} \in \mathbb{R}^{d_{model} \times d_{k}}, W^{K}_{i} \in \mathbb{R}^{d_{model} \times d_{k}}, W^{V}_{i} \in \mathbb{R}^{d_{model} \times d_{k}}</script>，而<script type="math/tex">W^{O}_{i} \in \mathbb{R}^{hd_{v} \times d_{model}}</script></p>
<p>在transformer中使用h=8个parallel attention heads.对于每个head使用<script type="math/tex">d_k=d_v=d_{model}/h=64</script>. 我看<a href="https://www.tensorflow.org/tutorials/text/transformer#encoder_layer" target="_blank" rel="noopener">tf official tutorial of transformer</a>实现中并没有为<script type="math/tex">head_i</script>学习转换矩阵<script type="math/tex">W^{Q}_{i},W^{K}_{i},W^{V}_{i}</script>，而是先对<script type="math/tex">(Q, K, V)</script>作一个线性转换到<script type="math/tex">d_{model}</script>的编码，然后将<script type="math/tex">d_{model}</script>的编码表示划分成h份，然后输入到各自的head中进行处理。<font color='red'>Multi-Head Attention机制允许模型在不同位置共同关注来自不同表示子空间的信息。</font></p>
<p>Decoder中包含两层的Multi-Head Attention(MHA)，第二层的MHA不再使用初始的<script type="math/tex">(Q, K, V)</script>作为输入，而是使用上一层MHA的输出作为<script type="math/tex">Q</script>，Encoder最后一层(第6层)的输出作为<script type="math/tex">K, V</script>.</p>
<h3 id="Masking"><a href="#Masking" class="headerlink" title="Masking"></a>Masking</h3><p>无论encoder还是decoder都需要使用mask屏蔽掉不必要的信息干扰。</p>
<p>对于encoder，由于我们传入的数据是padded batch，因此需要对padded的信息进行mask，具体mask的位置放置在dot-producted attention的scale即<script type="math/tex">\frac{Qk^T}{\sqrt{d_k}}</script>之后softmax之前，对需要mask的位置赋值为-1e9，这样在softmax之后需要mask的位置就变成了0，就像是指定mask的位置进行dropout.</p>
<p>对于decoder，每一层包含两个Multi-Head Attention(MHA)，第一个MHA同样需要对padded信息进行mask，除此之外还要对output中还未出现的词进行mask，保证只能根据前面出现的词信息来预测后面还未出现的词。将两个mask矩阵叠加之后输入到dot-producted attention单元中的scale之后用于屏蔽非必要信息。第二个MHA由于使用Encoder最后一层的输出作为<script type="math/tex">K, V</script>，所以需要对Encoder初始输入的padded信息进行mask，同样是在dot-producted attention单元中的scale之后操作。</p>
<h3 id="Embeddings"><a href="#Embeddings" class="headerlink" title="Embeddings"></a>Embeddings</h3><p>encoder和decoder的输入是两套独立的embedding(例如translation task，分别是原语和目标语subword的Embeddings)，embedding的维度为<script type="math/tex">d_{model}</script>，论文中的base model设为512，big model设为1024.在embedding输入模型前逐元素乘以<script type="math/tex">\sqrt{d_{model}}</script>，因为在dot-producted attention中 <script type="math/tex">\frac{QK^T}{\sqrt{d_{model}}}</script>.</p>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>由于transform model不包含循环和卷积网络，为了使模型能利用sequence的顺序信息，必须加入序列中每个subword的相对或者绝对位置信息。因此引入了Positional Encoding，它保持和embedding一样维度<script type="math/tex">d_{model}</script>，具体encoding的方式有很多种，主要分为学习的和固定的(learned and fixed)。在论文中使用了固定的无须学习的positional embedding，使用不同频率的cos和sin函数如下</p>
<script type="math/tex; mode=display">
\begin{align}
PE_{(pos,2i)}&=sin(pos/10000^{2i/d_{model}})\\
PE_{(pos,2i+1)}&=cos(pos/10000^{2i/d_{model}})
\end{align}</script><p>其中，pos是token position in sequence，i是dimension position.之所以选择cos和sin函数是因为对于固定的偏置量<script type="math/tex">k,PE_{pos+k}</script>能够表示为<script type="math/tex">PE_{pos}</script>的线性函数，假设它可以很容易学习到相对位置引入的信息。使用cos和sin的另一个原因是它允许模型推断比训练过程中遇到的更长的序列。论文中作者表示也使用了learned positional embeddings，但是与上述的fixed positional embeddings结果基本一致，所以最终采用了fixed positional embedings，这种方式更高效，减少训练开销。</p>
<p>最终作为模型输入的是token embeddings + positional embeddings，然后套一层dropout.</p>
<h3 id="Why-Self-Attention"><a href="#Why-Self-Attention" class="headerlink" title="Why Self-Attention"></a>Why Self-Attention</h3><div aling="center">
    <img src="/images/NLP/transformer_3.png">
</div>

<p>如上图所示，使用self-attention机制主要有三点原因</p>
<ul>
<li>相比convolutional和recurrent，当sequence length n比representation dimensionality d更小时(SOTA的机器翻译模型都符合)，每一层的计算开销都更少</li>
<li>self-attention相比recurrent和convolutional更适合并行化，可并行化的粒度更小(measured by the minimum sequential operations required)</li>
<li>最重要的一点self-attention解决了长距离依赖(long -range denpendency)的问题。对于sequence transduction task，影响这种依赖的关键因素就是前向或者后向信号要在网络中遍历的路径长度。而self-attention的Q(query)直接与K(key),V(value)中每个token直接产生联系，无须信号序列式传递，所以如上图中self-attention的sequential operations和maximum path length常数级别的。</li>
</ul>
<p>由于self-attention的complexity per layer为<script type="math/tex">O(n^2 \cdot d)</script>，考虑非常长序列的极限情况，可以限制self-attention在计算时只考虑输入序列K,V中的r个邻居即可将complexity per layer限制在<script type="math/tex">O(r \cdot n \cdot d)</script>，这就是上图2中的Self-Attention(restricted).</p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>WMT 2014 English-German datasset和WMT 2014 English-French dataset，数据集<a href="http://www.statmt.org/wmt14/translation-task.html" target="_blank" rel="noopener">官网download</a>，论文使用其中的newstest2013作为验证集(development dataset)，newstest2014作为测试集(test dataset)。base model平均最后5个checkpoint的结果，big model平均最后20个checkpoint的结果。由于数据集太过庞大，训练代价很大。论文中使用8 NVIDIA P100 GPUs，base model训练100000 steps耗时12h，big model训练300000 steps耗时3.5 days.</p>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>论文中主要使用了两种正则化手段来避免过拟合并加速训练过程。</p>
<h4 id="Residual-Dropout"><a href="#Residual-Dropout" class="headerlink" title="Residual Dropout"></a>Residual Dropout</h4><p>在每一residual Multi-Head Attention之后，Add&amp;Norm之前进行dropout，以及add(token embedding,positional encoding)之后进行dropout，FFN中没有dropout，base model的dropout rate统一设置为0.1，big model在wmt14 en-fr数据集上设置为0.1，在en-de数据集上设置为0.3</p>
<h4 id="Label-Smothing"><a href="#Label-Smothing" class="headerlink" title="Label Smothing"></a>Label Smothing</h4><p>Label Smothing Regularization(LSR)是2015年发表在CoRR的<a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="noopener">paper:Rethinking the inception architecture for computer vision</a>中的一个idea，这个idea简单又实用。假设数据样本x的针对label条件概率的真实分布为</p>
<script type="math/tex; mode=display">
q(k|x)=\delta_{k,y}=\left\{
\begin{aligned}
1, k = y\\
0, k \neq y
\end{aligned}
\right.</script><p>这使得模型对自己给出的预测太过自信，容易导致过拟合并且自适应能力差(easy cause overfit and hard to adapt)。解决方案：给label分布加入平滑分布<script type="math/tex">u(k)</script>，一般取均匀分布<script type="math/tex">u(k)=\frac{1}{k}</script>就好，于是得到</p>
<script type="math/tex; mode=display">
q'(k|x)=(1-\epsilon)\delta_{k,y}+\epsilon u(k)</script><p>映射到损失函数cross entropy有</p>
<script type="math/tex; mode=display">
\begin{aligned}
H(q',p)&=-\sum_{k=1}^{K}\log^{p(k)}q'(k)\\
&=(1-\epsilon)H(q,p)+\epsilon H(u,p)
\end{aligned}</script><p>由上式可知，LSR使得不仅要最小化原来的交叉熵H(q,p)，还要考虑预测分布<script type="math/tex">p</script>与<script type="math/tex">u(k)</script>之间差异最小化，使得模型预测泛化能力更好。transformer的论文中指定<script type="math/tex">\epsilon_{ls}=0.1</script>。下表是使用LSR和未使用LSR在tensorflow datasets的ted_hrlr_translate/pt_to_en dataset上bleu score对比</p>
<center>表1. bleu score with LSR and without LSR</center>


<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>bleu on validation dataset</th>
<th>bleu on test dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td>beam_search</td>
<td>0.415/41.5</td>
<td>0.420/42.0</td>
</tr>
<tr>
<td>beam_search + label_smooth_regualrization</td>
<td>0.473/47.3</td>
<td>0.468/46.8</td>
</tr>
</tbody>
</table>
</div>
<p>可以看到使用了LSR在验证集和测试集上都取得了比更好的bleu score.但是LSR对perplexity不利，因为模型的学习目标变得更不确切了。</p>
<h3 id="LayerNormalization"><a href="#LayerNormalization" class="headerlink" title="LayerNormalization"></a>LayerNormalization</h3><p>在multi-head attention之后使用layer normlization可以加速参数训练使得模型收敛，并且可以避免梯度消失和梯度爆炸。相比BatchNormalization，LayerNormalization更适用于序列化模型比如RNN等，而BatchNormalization则适用于CNN处理图像。</p>
<h3 id="Learning-Rate-with-Warmup"><a href="#Learning-Rate-with-Warmup" class="headerlink" title="Learning Rate with Warmup"></a>Learning Rate with Warmup</h3><div align="center">
    <img src="/images/NLP/transformer_4.png">
</div>

<p>Transformer使用Adam optimizer with <script type="math/tex">\beta_{1}=0.9,\beta_{2}=0.98,\epsilon=10^{-9}</script>.学习率在训练过程中会变动，先有一个预热，学习率呈线性增长，然后呈幂函数递减如上图所示，下面是学习率的计算公式</p>
<script type="math/tex; mode=display">
lrate=d^{-0.5}_{model} \cdot min(step\_num^{-0.5},step\_num \cdot warmup\_steps^{-1.5})</script><p>论文中设置warmup_steps=4000.也就是说训练的前4000步线性增长，4000步后面呈幂函数递减。这么做可以加速模型训练收敛，先以上升的较大的学习率让模型快速落入一个局部收敛较优的状态，然后以较小的学习率微调参数慢慢逼近更优的状态以避免震荡。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Transformer是NLP在深度学习发展历程上的一座里程碑，目前主流的预训练语言模型都是基于transformer的，逐渐取代了LSTM的位置，深入理解transformer的细节对后续NLP的学习非常重要。</p>
<p>Transformer最大的亮点在于不依靠RNN和CNN，通过引入self-attention机制，很好地解决了让人头疼的长距离依赖问题，使得输入和输出直接关联，没有了RNN那样的序列传递信息损失，输入输出之间经过的的路径长为常数级，与输入序列长度无关，上下文信息保留更完整。因此tranformer是非常强大的文本生成模型，应用于MT, 成分句法分析(constituency parsing)等效果非常好。</p>

          
        
      
    </div>
    
    
    
    
    <!-- 文章结束分割线 -->
    <div style="text-align:center;color: #ccc;font-size:14px;">---------------- The End ----------------</div>

    

    

    
    
    <div>
	    <p id="div-border-left-red">
	    <span>
        <b>作者: brooksjay</b><br/>
        <b>联系邮箱: jaypark@smail.nju.edu.cn</b><br/>
	    <b>本文地址: </b><a href="/index.html" title=""></a><br/>
	    <b>本文基于<a target="_blank" title="Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)" href="http://creativecommons.org/licenses/by-sa/4.0/"> 知识共享署名-相同方式共享 4.0 </a>国际许可协议发布</b><br/>
        <b>转载请注明出处, 谢谢！</b>
	    </span>
	    </p>
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      


  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">

    <link itemprop="mainEntityOfPage" href="https://brooksj.com/2019/12/01/Standford-CS224n-Notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="brooksj">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="在人间漂流">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/12/01/Standford-CS224n-Notes/" itemprop="url">Standford CS224n Notes</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-12-01T22:06:28+08:00">
                2019-12-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/12/01/Standford-CS224n-Notes/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="https://brooksj.com/2019/12/01/Standford-CS224n-Notes/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          
          
             <span id="/2019/12/01/Standford-CS224n-Notes/" class="leancloud_visitors" data-flag-title="Standford CS224n Notes">
               <span class="post-meta-divider"><br/></span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          <span class="post-time">
	        &nbsp; | &nbsp;
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">字数统计:</span>
            <span class="post-count">1.2k(字)</span>
            
          </span>
	  
          <span class="post-time">
	        &nbsp; | &nbsp;
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">阅读时长:</span>
            <span class="post-count">4(分)</span>
          </span>

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Lecture-08-Machine-Translation-Seq2Seq-and-Attention"><a href="#Lecture-08-Machine-Translation-Seq2Seq-and-Attention" class="headerlink" title="Lecture 08 Machine Translation, Seq2Seq and Attention"></a>Lecture 08 Machine Translation, Seq2Seq and Attention</h2><h3 id="Statistical-Machine-Translation-SMT"><a href="#Statistical-Machine-Translation-SMT" class="headerlink" title="Statistical Machine Translation(SMT)"></a>Statistical Machine Translation(SMT)</h3><p>统计机器翻译(Statistical Machine Translation，SMT)出现并流行于1990s～2010s，1990s以前的MT工作大部分都是基于规则的(rule-based)，使用双语词典作为映射，最早可以追溯到1950s。SMT的main idea如下图所示</p>
<div align="center">
    <img src="/images/NLP/lecture8_1.png">
</div>

<p>SMT将模型划分为2部分进行学习，左部主要用于确保翻译后的句义保真度，尽可能学习到原句的真实含义。而右部主要用于确保翻译后句子的流畅度。机器翻译还有一个难题就是对齐(alignment)，原句和目标语句之间的语法可能差异很大，使得词序排列分布差异很大，要学习原句词与目标句词之间的对齐方式是一件非常棘手的事。定位alignment关系非常复杂，因为alignment有一对多、多对一以及多对多的关系，SMT通过加入一隐变量a用于专门学习alignment，即将$P(x|y)$进一步划分为$P(x,a|y)$，其中a物理含义是原句与目标句之间word-level correspodence.</p>
<h3 id="Nueral-Machine-Translation-NMT"><a href="#Nueral-Machine-Translation-NMT" class="headerlink" title="Nueral Machine Translation(NMT)"></a>Nueral Machine Translation(NMT)</h3><h4 id="seq2seq-model"><a href="#seq2seq-model" class="headerlink" title="seq2seq model"></a>seq2seq model</h4><p>seq2seq model的结构如下图所示</p>
<div align="center">
    <img src="/images/NLP/lecture8_2.png">
</div>

<p>从上图看seq2seq应用于NMT的方式一目了然，这是一个auto-regressive model，通过历史序列信息学习预测当前词，loss为交叉熵。seq2seq无须对输入数据进行预特征提取，直接将原始序列输入即可得到目标序列输出，一个典型的端到端黑箱模型。正由于其简便易用，得到了很多应用，seq2seq model还能应用于</p>
<div align="center">
    <img src="/images/NLP/lecture8_3.png">
</div>

<div align="center">
    <img src="/images/NLP/transformer_2.png">
</div>

<div align="center">
    <img src="/images/NLP/lecture8_4.png">
</div>

<p>一般beam size设置越大，寻找到的结果更优，但不一定是最优，只有搜索所有路径才可以得到最优解，一直搜索直到预测到结束标记，概率值为language model的log probability的前缀和，因此序列越长log probability的和可能越小，结果要除以对应序列长度来比较候选序列。通常可以设置概率阈值、候选序列个数、预测序列长度限制来作为搜索的终止条件。由于beam search容易使用来自训练集中的常见短语和重复文本，因此<a href="https://arxiv.org/pdf/1805.04833.pdf" target="_blank" rel="noopener">Hierarchical Neural Story Generation</a>这篇文章提出使用<em>top-k random sample scheme</em>的方法，简单概括就是先选出LM生成概率最高的k个词，然后从这top-k个词中随机采样一个词作为下一个序列生成词，重复上述步骤直到生成终止词。 MT task的评估一般采用BLEU，可以参考assignment4 handout或者<a href="https://dl.acm.org/citation.cfm?id=1073135" target="_blank" rel="noopener">paper bleu</a>，NLTK提供了基于句子和语料库计算<a href="http://www.nltk.org/api/nltk.translate.html##nltk.translate.bleu_score.sentence_bleu" target="_blank" rel="noopener">bleu score的API</a>，<font color='red'>注意计算整个corpus的bleu不是求corpus中每个句子bleu的均值，而是累加每个句子的ngrams统计信息到同一个分子分母中统一计算得来。</font>bleu取值范围[0,1]，但是论文中都习惯将bleu乘以100后保留一位小数来表示。</p>
<p>NMT相比SMT的优点如下</p>
<div align="center">
    <img src="/images/NLP/lecture8_5.png">
</div>

<p>NMT缺点在于不可解释的黑箱，难以调试，有结果超乎预料的风险。</p>
<h4 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h4><p>seq2seq model简便以用，但是瓶颈也很明显如下图所示</p>
<div align="center">
    <img src="/images/NLP/lecture8_6.png">
</div>

<p>最后一个词输出的hidden state需要囊括句子的所有信息，而LSTM对于长距离问题又无法很好地解决，因此最终的hidden state肯定丢失了很多的信息。为了能很好的提取前面所有词的编码信息，需要引入注意力机制</p>
<p>core idea: on each step of the decoder, use direct connection to the encoder to focus on a particular part of the source sequence</p>
<p>seq2seq引入attention的具体方式如下所示</p>
<div align="center">
    <img src="/images/NLP/lecture8_7.png">
</div>

<p>如上，将原句编码后的hidden state向量与原句中每个词的编码向量作点积运算得到每个词的attention score(scalar)，然后softmax所有词的attention scores得到attention distribution(上图可以看到attention在下一相关词上分布最多)，然后再用attention distribution对词编码向量作加权求和得到attention output，然后concat到decoder的hidden state预测下一个词。下图是下一层的执行方式</p>
<div align="center">
    <img src="/images/NLP/lecture8_8.png">
</div>



<p><font color='red'>将前一层的attention output和前一层的预测输出词向量concat作为这一层的输入</font>，由于此时hidden state是拼接的要取原句子编码向量那一段输入到attention机制中，得到的attention output同样concat到当前的句子编码中用于预测下一词。</p>
<p>attention正是通过直接与句子中所有word embedding进行点积(direct connection)，并将attention distribution用于句子中所有词向量的加权求和从而解决了LSTM无法解决的长距离依赖问题，使得hidden state包含了句子更丰富的信息，从而能更好得预测下一个词。</p>
<p>attention不仅是用于seq2seq model，它有更general的用途如下</p>
<div align="center">
    <img  src="/images/NLP/lecture8_9.png">
</div>

<p>attention mechanism有三种常见变体形式如下</p>
<div align="center">
    <img src="/images/NLP/lecture8_10.png">
</div>

<div align="center">
    <img src="/images/NLP/lecture8_11.png">
</div>






          
        
      
    </div>
    
    
    
    
    <!-- 文章结束分割线 -->
    <div style="text-align:center;color: #ccc;font-size:14px;">---------------- The End ----------------</div>

    

    

    
    
    <div>
	    <p id="div-border-left-red">
	    <span>
        <b>作者: brooksjay</b><br/>
        <b>联系邮箱: jaypark@smail.nju.edu.cn</b><br/>
	    <b>本文地址: </b><a href="/index.html" title=""></a><br/>
	    <b>本文基于<a target="_blank" title="Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)" href="http://creativecommons.org/licenses/by-sa/4.0/"> 知识共享署名-相同方式共享 4.0 </a>国际许可协议发布</b><br/>
        <b>转载请注明出处, 谢谢！</b>
	    </span>
	    </p>
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      


  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">

    <link itemprop="mainEntityOfPage" href="https://brooksj.com/2019/11/26/Subword-Models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="brooksj">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="在人间漂流">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/26/Subword-Models/" itemprop="url">Subword Models</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-26T21:03:48+08:00">
                2019-11-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/11/26/Subword-Models/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="https://brooksj.com/2019/11/26/Subword-Models/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          
          
             <span id="/2019/11/26/Subword-Models/" class="leancloud_visitors" data-flag-title="Subword Models">
               <span class="post-meta-divider"><br/></span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          <span class="post-time">
	        &nbsp; | &nbsp;
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">字数统计:</span>
            <span class="post-count">2.3k(字)</span>
            
          </span>
	  
          <span class="post-time">
	        &nbsp; | &nbsp;
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">阅读时长:</span>
            <span class="post-count">9(分)</span>
          </span>

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>随着深度神经网络的发展，深度学习在文本生成比如nueral machine translation(NMT)以及question answer(QA)等任务中突破了传统方法的瓶颈，在很多数据集上都取得了state-of-the-art(SOTA)的效果。</p>
<p>我们知道神经网络输入层必须接收数字形式的信息，所以必须将文本形式的词嵌入成向量。通常会设定一个词汇表大小，对于词汇表中所有词有对应的向量表示。而由于一种语言涵盖的词汇太多，以及处理的文本数据本身就不规范(比如web数据)，从而无法覆盖所有的词汇，在对corpus中的词进行向量map时会出现out of  vocabulary(OOV)的现象。以往的做法是设置一个特殊字段(比如UNK)来表示所有OOV的词汇，这种做法可谓相当粗糙，那有没有更细致微妙的处理方法呢？答案当然是有的，很多从事NLP研究的工作者在这个方面开展了不少的工作。一种方法是从word embedding扩展到character embedding，另外一种方法是将word拆分成有意义的更小的词素也就是wordpieces，两个word可能不相同，但是他们可能包含相同的wordpieces，这样对于OOV的word，找到其对应的wordpieces的向量added or averaged也可以蕴含该word的word sense。接下来分别介绍这两种方法。</p>
<h2 id="Character-Embedding"><a href="#Character-Embedding" class="headerlink" title="Character Embedding"></a>Character Embedding</h2><p>在没出现character embedding之前可能会觉得仅用少量的字符向量就可以表示含有丰富含义的词向量会有点不可思议，但借助于神经网络对特征强大的组合能力，使得这一切成为了可能。目前对word中的characters进行组合的方式目前主要有convolutional network和LSTM network两种。下面分别介绍这两种方法以及混合使用word-character embedding的模型。</p>
<h3 id="Convolutional"><a href="#Convolutional" class="headerlink" title="Convolutional"></a>Convolutional</h3><p><a href="http://proceedings.mlr.press/v32/santos14.pdf" target="_blank" rel="noopener">Learning Character-level Representations for Part-of-Speech Tagging C´ıcero</a>，这篇文章发表于JMLR2014，应该是最早的使用卷积神经网络处理char embedding的，模型主要分三层conv+fcn+maxpool，具体的结构图如下</p>
<div align="center">
    <img src="/images/NLP/cnn_char_embedding.png">
</div>

<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p><a href="https://arxiv.org/abs/1508.02096" target="_blank" rel="noopener">Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation</a>，这篇文章发表于EMNLP2015，使用LSTM网络处理char embedding，并将其很好得应用于language model和POS tagging，具体的模型结构如下</p>
<div align="center">
    <img src="/images/NLP/lstm_char_embedding.png">
</div>

<p>模型使用双向LSTM对character embedding进行处理并将两个方向最终的隐状态向量拼接作为word embedding输出。</p>
<h3 id="Hybrid-Word-Character-Models"><a href="#Hybrid-Word-Character-Models" class="headerlink" title="Hybrid Word-Character Models"></a>Hybrid Word-Character Models</h3><p>character embedding与word embedding不同在于：word embedding是在巨大语料集上学习好的，然后再用于NLP的各种下游任务，而character embedding是随机初始化的，在学习下游任务目标的同时学习character embedding。考虑到word embedding和character embedding各自都包含了有用的信息，因此Christopher D. Manning等人提出了综合使用word embedding和character embedding的混合模型<a href="https://arxiv.org/abs/1604.00788" target="_blank" rel="noopener">Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models</a>，这篇文章发表于ACL2016，模型使用单向LSTM网络，具体结构如下</p>
<div align="center">
    <img src="/images/NLP/hybrid_word_char_embedding.png">
</div>

<p>对于输入序列中的OOV word使用双层LSTM处理其对应的char embedding得到的输出作为word embedding，对于输出序列中的\<unk\>使用双层LSTM+beam search转换成有意义的word。</p>
<h2 id="WordPieces"><a href="#WordPieces" class="headerlink" title="WordPieces"></a>WordPieces</h2><p>wordpieces这个名字取得很形象，说白了就是将词划分成片表示，但可不是随便划分。目前的划分方法主要有两种：</p>
<ul>
<li>基于频率计数的方法byte pair encoding(BPE)</li>
<li>基于unigram language model的方法</li>
</ul>
<h3 id="byte-pair-encoding"><a href="#byte-pair-encoding" class="headerlink" title="byte pair encoding"></a>byte pair encoding</h3><p><a href="https://arxiv.org/abs/1508.07909" target="_blank" rel="noopener">Neural Machine Translation of Rare Words with Subword Units</a>，这篇文章发表于ACL2016。文章思路其实很简单，初始化所有的wordpieces为单个字符，就是统计相邻两个wordpieces在所有word中共现频率，然后合并出现频率最高的wordpieces，直到当前词汇表满足设定的大小。注意，合并起来的wordpieces的所有前缀都要包含进词汇表中。下面是BPE算法的python代码</p>
<div align="center">
    <img src="/images/NLP/bpe_algorithm.png">
</div>

<p>代码一目了然，但正是这么简单的方法却在很多NLP下游任务上取得了很不错的效果，因而得到了广泛的应用，毕竟简单而又好用的谁不喜欢呢！！</p>
<h3 id="unigram-language-model"><a href="#unigram-language-model" class="headerlink" title="unigram language model"></a>unigram language model</h3><p><a href="https://arxiv.org/abs/1804.10959" target="_blank" rel="noopener">Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates</a>，这篇文章发表于ACL2018。文章基于输入语料库建立unigram model，使用EM算法迭代，每次保留去除该词后使得语言模型的perpelxity损失最大的前80%的词，迭代计算直到当前词汇表满足设定的大小。</p>
<h3 id="Google’s-SentencePiece"><a href="#Google’s-SentencePiece" class="headerlink" title="Google’s SentencePiece"></a>Google’s SentencePiece</h3><p><a href="https://arxiv.org/abs/1808.06226" target="_blank" rel="noopener">SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</a>，这篇文章发表于EMNLP2018，在发表这篇文章之前，google也发表了一篇基于unigram language model的建立wordpieces词汇表的文章<a href="https://ieeexplore.ieee.org/abstract/document/6289079" target="_blank" rel="noopener">Japanese and Korean voice search</a>，这篇文章发表于IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)2012。但SentencePiece这篇文章可谓集大成者，Google在这篇文章基础上开源了<a href="https://github.com/google/sentencepiece" target="_blank" rel="noopener">github: SentencePiece Package</a>，它不仅包含常用的character和word embedding，并且实现了前述2种subword segmentation algorithms即BPE和unigram language model，Google真乃良心啊，这就为后面所有NLPer的工作提供了提供了一个统一的输入预处理工具。并且该工具还可以保存构建wordpieces的model，便于论文实验结果的再现(reproducibility，因为很多NLP模型对输入极其敏感，输入稍有偏差将导致结果迥然不同)。SentencePiece Package上手使用非常简单，下面给出代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sentencepiece <span class="keyword">as</span> spm</span><br><span class="line"><span class="comment"># model_prefix是模型输出2个文件*.vocab，*.model前缀名</span></span><br><span class="line"><span class="comment"># model_type模型种类:unigram, bpe, char, word</span></span><br><span class="line"><span class="comment"># 为了支持跨语言处理，SentencePiece支持自定义character normalization，默认是使用NFKC的unicode编码转换</span></span><br><span class="line"><span class="comment"># input文件内容每行一个sentence，model_type为word时需要提前分好词，其它3种不需要，原始输入即可</span></span><br><span class="line">spm.SentencePieceTrainer.Train(<span class="string">'--input=newtext.txt \</span></span><br><span class="line"><span class="string">								--normalization_rule_name=nfkc \</span></span><br><span class="line"><span class="string">                               --model_prefix=subword \</span></span><br><span class="line"><span class="string">                               --vocab_size=1000 \</span></span><br><span class="line"><span class="string">                               --model_type=unigram'</span>)</span><br></pre></td></tr></table></figure>
<p>Train方法中的参数说明如下</p>
<ul>
<li><code>--input</code>: one-sentence-per-line <strong>raw</strong> corpus file. No need to run tokenizer, normalizer or preprocessor. By default, SentencePiece normalizes the input with Unicode NFKC, . You can pass a comma-separated list of files.</li>
<li><code>--model_prefix</code>: output model name prefix. <code>.model</code> and <code>.vocab</code> are generated.</li>
<li><code>--vocab_size</code>: vocabulary size, e.g., 8000, 16000, or 32000</li>
<li><code>--character_coverage</code>: amount of characters covered by the model, good defaults are: <code>0.9995</code> for languages with rich character set like Japanse or Chinese and <code>1.0</code> for other languages with small character set.</li>
<li><code>--model_type</code>: model type. Choose from <code>unigram</code> (default), <code>bpe</code>, <code>char</code>, or <code>word</code>. The input sentence must be pretokenized when using <code>word</code> type.</li>
</ul>
<p>执行上面代码后会在当前目录下生成subword.vocab和subword.model两个文件，下面给出加载模型进行编码解码的代码示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sentencepiece <span class="keyword">as</span> spm</span><br><span class="line"></span><br><span class="line">sp = spm.SentencePieceProcessor()</span><br><span class="line">sp.Load(<span class="string">'unigram.model'</span>)</span><br><span class="line">ids = sp.EncodeAsIds(<span class="string">'I love you'</span>)</span><br><span class="line">print(ids)</span><br><span class="line">print(sp.DecodeIds(ids))</span><br></pre></td></tr></table></figure>
<h3 id="fastText-embedding"><a href="#fastText-embedding" class="headerlink" title="fastText embedding"></a>fastText embedding</h3><p>wordpieces以很巧妙的思想解决了OOV的问题，后面出现了大量的使用wordpieces作为embedding的工作。这里不得不提一下<a href="https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00051" target="_blank" rel="noopener">fastText: Enriching Word Vectors with Subword Information</a>，这篇文章是facebook ai团队发表于TACL2017，这篇文章使用和word2vector一样的学习embedding的方法skip-gram，但是它是学习word的subword n-grams(文章中n设为3~6，就是word的所有连续的3~6grams) embedding，这样其它任务使用词向量出现OOV现象即可使用该词n-grams的subword embeddig累加来合成。那么如何训练subword n-grams 的embedding呢？就skip-gram进行讨论，假设center word为$w_t$，window word为$w_c$，则围绕$w_t$的loss为</p>
<script type="math/tex; mode=display">
log(1+e^{-s(w_{t},w_{c})})+\sum_{n  \in \mathcal{N}_{t,c}}{log(1+e^{s(w_{t},w_{c})})}</script><p>其中，$\mathcal{N}<em>{t,c}$ is a set of negative examples sampled from the vocabulary.  我们用$\mathcal{G}w \subset {1,…,G}$  表示出现在w中的n-grams集合(3~6 grams in paper)，$z</em>{g}$表示$w$每个n-gram的embedding，$w$的向量表示为$z_g$的和，因此可得$s(w,c)$</p>
<script type="math/tex; mode=display">
s(w,c)=\sum_{g \in \mathcal{G}_w}z_{g}^{T}v_{c}</script><p>该方法异常高效，train cost非常低，不同以往的word embedding学习需要海量的文本数据进行学习，fastText只需要word2vec方法的5%甚至1%的数据即可在许多任务上达到一致的效果。下图是word2vec的cbow方法以及fastText的skip-gram方法使用不同大小数据集训练后在word similarity任务上spearman rank表现对比</p>
<div align="center">
    <img src="/images/NLP/fastText_data_percent.png">
</div>

<p>其中，sisg-是fastText使用null vector表示OOV word，而sisg是fastText使用summing subword n-grams表示OOV word，可以看到sisg使用少量数据之后即可效果基本就饱和了。fastText对于复合词多、词尾变化丰富、多形态词的语言，以及包含大量生僻词的语料库效果更为明显，因为这些很难直接用word embedding直接覆盖，非常适合使用subword n-grams来拼接表示。但是对于常见词使用直接word embedding效果会比subword n-grams embedding效果更好，如下图所示，可以看到在word similarity任务上，fastText在WS353 dataset上表现不如word2vec的cbow，because words in the English WS353 dataset are common words for which good vectors can be obtained without exploiting subword information.</p>
<div align="center">
    <img src="/images/NLP/fastText1.png">
</div>



<p>所以前面提到的<a href="https://arxiv.org/abs/1604.00788" target="_blank" rel="noopener">Hybrid Word-Character Model</a>也正是发现此问题后的进一步优化，能优先使用word embedding的就是用word embedding，否则使用character embedding或者wordpieces embedding。</p>

          
        
      
    </div>
    
    
    
    
    <!-- 文章结束分割线 -->
    <div style="text-align:center;color: #ccc;font-size:14px;">---------------- The End ----------------</div>

    

    

    
    
    <div>
	    <p id="div-border-left-red">
	    <span>
        <b>作者: brooksjay</b><br/>
        <b>联系邮箱: jaypark@smail.nju.edu.cn</b><br/>
	    <b>本文地址: </b><a href="/index.html" title=""></a><br/>
	    <b>本文基于<a target="_blank" title="Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)" href="http://creativecommons.org/licenses/by-sa/4.0/"> 知识共享署名-相同方式共享 4.0 </a>国际许可协议发布</b><br/>
        <b>转载请注明出处, 谢谢！</b>
	    </span>
	    </p>
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      


  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">

    <link itemprop="mainEntityOfPage" href="https://brooksj.com/2019/11/13/Backpropagation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="brooksj">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="在人间漂流">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/13/Backpropagation/" itemprop="url">Backpropagation</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-13T19:46:55+08:00">
                2019-11-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/11/13/Backpropagation/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="https://brooksj.com/2019/11/13/Backpropagation/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          
          
             <span id="/2019/11/13/Backpropagation/" class="leancloud_visitors" data-flag-title="Backpropagation">
               <span class="post-meta-divider"><br/></span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          <span class="post-time">
	        &nbsp; | &nbsp;
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">字数统计:</span>
            <span class="post-count">820(字)</span>
            
          </span>
	  
          <span class="post-time">
	        &nbsp; | &nbsp;
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">阅读时长:</span>
            <span class="post-count">3(分)</span>
          </span>

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>后向传播(backpropagation)算法在深度学习中扮演了非常重要的角色，它能够从损失函数开始链式地对网络层中的权重进行梯度计算与更新。可以先参考<a href="https://en.wikipedia.org/wiki/Backpropagation" target="_blank" rel="noopener">维基百科: Backpropagation</a>，文章很好地还原了深度神经网络(DNN)每一层网络从后往前的链式梯度计算关系。然后再结合下面这几张图理解每一层网络中每个单元的梯度具体计算与推导过程，假设前向计算公式$wx + b$.</p>
<div align="center">
    <img src="/images/backpropagation/bp1.png">
</div>

<div align="center">
    <img src="/images/backpropagation/bp2.png">
</div>

<div align="center">
    <img src="/images/backpropagation/bp3.png">
</div>

<div align="center">
    <img src="/images/backpropagation/bp4.png">
</div>

<p><div align="center">
    <img src="/images/backpropagation/bp5.png">
</div><br>举个例子</p>
<p><img src="/images/backpropagation/bp6.svg" align="center"></p>
<p>上图是一个两层的全连接神经网络，其中<script type="math/tex">net_{i}</script>是输入，<script type="math/tex">net_{k}</script>是输出，输出在softmax之后计算交叉熵损失。下面给出详细计算隐藏层<script type="math/tex">net_{j}</script>对应的权重<script type="math/tex">w_{ij}</script>梯度的过程，其中<script type="math/tex">N</script>是batch size，<script type="math/tex">y</script>是真实标签，<script type="math/tex">o</script>是softmax激活后的输出</p>
<script type="math/tex; mode=display">
E(w)=\frac{1}{N}\sum_{i=1}^{N}E_{p}(w)=\frac{1}{N}\sum_{i=1}^{N}\sum_{d=1}^{D}{-y_{id}\log{o_{id}}}\\
\begin{aligned}
\frac{\partial{E_{p}(w)}}{\partial{w_{ij}}}&=\frac{\partial{E_{p}(w)}}{\partial{net_{j}}} \cdot \frac{\partial{net_{j}}}{\partial{w_{ij}}}\\
&=x_{i}\sum_{k \in nextlayer(j)}\frac{\partial{E_{p}(w)}}{\partial{net_k}} \cdot \frac{\partial{net_k}}{net_{j}}\\
&=x_{i}\sum_{k \in nextlayer(j)}\frac{\partial{E_{p}(w)}}{\partial{net_{k}}} \cdot w_{jk}
\end{aligned}</script><p>又因为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial{E_{p}(w)}}{\partial{net_{k}}} &= \frac{\partial{E_{p}(w)}}{\partial{o_k}} \cdot \frac{\partial{o_k}}{\partial{net_k}} \\
&=\frac{1}{-o_k} \cdot o_k \cdot (1-o_k) \\
&=o_k - 1
\end{aligned}</script><p>所以</p>
<script type="math/tex; mode=display">
\frac{\partial{E_p(w)}}{\partial{w_{ij}}}=x_i\sum_{k \in nextlayer(j)}{((o_{k}-1) \cdot w_{jk})}</script><p>除此之外，还有一个问题，交叉熵损失函数计算值只与标签1对应的输出相关，那么标签0对应输出的相关权重就无需计算梯度并进行更新了么？跑了一下上图两层全连接神经网络的demo程序，发现无论输出对应标签是0还是1，其相关权重梯度都不为0。通过对权重梯度的核算发现，对于标签0对应输出，虽然在交叉熵损失函数计算结果中没有得到体现，但是在后向传播梯度计算过程中，会采用$-(1-y_0)\log{(1-O_0)}$作为0标签相关权重梯度计算的损失，这点类似于逻辑回归损失函数，其中$y_0$为0，$O_0$为对应softmax输出。demo程序如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">mymodel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(mymodel, self).__init__()</span><br><span class="line">        self.l1 = nn.Linear(<span class="number">10</span>, <span class="number">20</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        nn.init.constant_(self.l1.weight, <span class="number">0.1</span>)</span><br><span class="line">        self.l2 = nn.Linear(<span class="number">20</span>, <span class="number">2</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        nn.init.constant_(self.l2.weight, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        t = self.l1(x)</span><br><span class="line">        print(t)</span><br><span class="line">        y_out = self.l2(t)</span><br><span class="line">        print(y_out)</span><br><span class="line">        <span class="keyword">return</span> y_out</span><br><span class="line"></span><br><span class="line">x = torch.rand(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">y = torch.tensor([<span class="number">0</span>])</span><br><span class="line">m = mymodel()</span><br><span class="line">optimizer = torch.optim.SGD(m.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line">loss_func = nn.CrossEntropyLoss()</span><br><span class="line">print(x)</span><br><span class="line">print(m.l1.weight)</span><br><span class="line">print(m.l2.weight)</span><br><span class="line">y_out = m(x)</span><br><span class="line">loss = loss_func(y_out, y)</span><br><span class="line">print(loss)</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> m.named_parameters():</span><br><span class="line">    print(name, param.grad, param)</span><br></pre></td></tr></table></figure>
<p>Tips:</p>
<ol>
<li>在实际应用场景中，可能由于硬件设备落后而无法支持大batch的数据训练，但是现在的深度学习框架一般都会支持梯度累积，可以用多个mini batch模拟big batch的训练，假设累积步数为k，那么每个mini batch在后向梯度计算之前要将loss除以k再做后向梯度计算。</li>
</ol>

          
        
      
    </div>
    
    
    
    
    <!-- 文章结束分割线 -->
    <div style="text-align:center;color: #ccc;font-size:14px;">---------------- The End ----------------</div>

    

    

    
    
    <div>
	    <p id="div-border-left-red">
	    <span>
        <b>作者: brooksjay</b><br/>
        <b>联系邮箱: jaypark@smail.nju.edu.cn</b><br/>
	    <b>本文地址: </b><a href="/index.html" title=""></a><br/>
	    <b>本文基于<a target="_blank" title="Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)" href="http://creativecommons.org/licenses/by-sa/4.0/"> 知识共享署名-相同方式共享 4.0 </a>国际许可协议发布</b><br/>
        <b>转载请注明出处, 谢谢！</b>
	    </span>
	    </p>
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      


  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">

    <link itemprop="mainEntityOfPage" href="https://brooksj.com/2019/10/17/pssh%E7%B3%BB%E5%88%97%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="brooksj">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="在人间漂流">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/17/pssh%E7%B3%BB%E5%88%97%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/" itemprop="url">pssh系列命令详解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-17T20:43:55+08:00">
                2019-10-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/10/17/pssh%E7%B3%BB%E5%88%97%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="https://brooksj.com/2019/10/17/pssh%E7%B3%BB%E5%88%97%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          
          
             <span id="/2019/10/17/pssh%E7%B3%BB%E5%88%97%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/" class="leancloud_visitors" data-flag-title="pssh系列命令详解">
               <span class="post-meta-divider"><br/></span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          <span class="post-time">
	        &nbsp; | &nbsp;
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">字数统计:</span>
            <span class="post-count">1.1k(字)</span>
            
          </span>
	  
          <span class="post-time">
	        &nbsp; | &nbsp;
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">阅读时长:</span>
            <span class="post-count">4(分)</span>
          </span>

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>pssh提供OpenSSH和相关工具的并行版本。包括pssh，pscp，prsync，pnuke和pslurp。该项目包括psshlib，可以在自定义应用程序中使用。pssh是python写的可以并发在多台机器上批量执行命令的工具，它的用法可以媲美ansible的一些简单用法，执行起来速度比ansible快它支持文件并行复制，远程命令执行，杀掉远程主机上的进程等等。杀手锏是文件并行复制，，当进行再远程主机批量上传下载的时候，最好使用它。pssh用于批量ssh操作大批量机器；pssh是一个可以在多台服务器上执行命令的工具，同时支持拷贝文件，是同类工具中很出色的；比起for循环的做法，更推荐使用pssh！ (注意需要安装 python 2.4 或以上版本)</p>
<p>下面是直接从源码进行编译安装的步骤，安装过程很快</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget https://pypi.python.org/packages/60/9a/8035af3a7d3d1617ae2c7c174efa4f154e5bf9c24b36b623413b38be8e4a/pssh-2.3.1.tar.gz</span><br><span class="line">tar xf pssh-2.3.1.tar.gz -C /usr/local</span><br><span class="line">cd /usr/local/pssh-2.3.1/</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure>
<h2 id="命令详解"><a href="#命令详解" class="headerlink" title="命令详解"></a>命令详解</h2><p><code>pssh --help</code>可以查看命令参数选项：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">-l    远程机器的用户名</span><br><span class="line">-p    一次最大允许多少连接</span><br><span class="line">-o    输出内容重定向到一个文件</span><br><span class="line">-e    执行错误重定向到一个文件</span><br><span class="line">-t    设置命令执行的超时时间</span><br><span class="line">-A   提示输入密码并且把密码传递给ssh（注意这个参数添加后只是提示作用，随便输入或者不输入直接回车都可以，可以结合sshpass -p password使用）</span><br><span class="line">-O   设置ssh参数的具体配置，参照ssh_config配置文件</span><br><span class="line">-x   传递多个SSH 命令，多个命令用空格分开，用引号括起来</span><br><span class="line">-X   同-x 但是一次只能传递一个命令</span><br><span class="line">-i   显示标准输出和标准错误在每台host执行完毕后</span><br><span class="line">-I   读取每个输入命令，并传递给ssh进程 允许命令脚本传送到标准输入</span><br></pre></td></tr></table></figure>
<p>pssh、pscp、prsync、pnuke和pslurp的具体使用：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">注:在使用工具前,确保主机间做了密钥认证,否则无法实现自动化,当然我们可以使用sshpass(yum install sshpass)配合pssh -A参数实现自动输入密码,但这要保证多台主机的密码相同,同时还要注意如果known_hosts没有信任远程主机,那么命令执行会失败,可以加上-O StrictHostKeyChecking=no参数解决,ssh能用的选项pssh也能用</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 集群刚装好系统处于原始状态，可以使用下面命令来生成其他机器的ssh秘钥并将各机器的rsa公钥添加到本机</span></span><br><span class="line">sshpass -p password pssh -I -A -O StrictHostKeyChecking=no -h ip.txt -l brooksj -i "ssh-ketgen"  # 然后本机回车10次帮助各机器生成ssh秘钥，password为其它机器的统一密码</span><br><span class="line">sshpass -p password pssh -A -O StrictHostKeyChecking=no -h ip.txt -l brooksj -i "ssh-copy-id localhost-ip" # localhost-ip改成你本机的ip</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash">pssh 远程批量执行命令 </span></span><br><span class="line">pssh -h ip.txt -P "uptime" </span><br><span class="line"><span class="meta">#</span><span class="bash">-h  后面接主机ip文件,文件数据格式[user@]host[:port]</span></span><br><span class="line"><span class="meta">#</span><span class="bash">-P  显示输出内容</span></span><br><span class="line"><span class="meta">#</span><span class="bash">如果没办法密钥认证.可以采用下面方法,但不是很安全</span></span><br><span class="line">sshpass -p 123456 pssh -A -h ip.txt -i "uptime"</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash">pscp 并行传输文件到远端</span></span><br><span class="line"><span class="meta">#</span><span class="bash">传文件,不支持远程新建目录</span></span><br><span class="line">pscp -h ip.txt test.py /tmp/dir1/</span><br><span class="line"><span class="meta">#</span><span class="bash">传目录</span></span><br><span class="line">pscp -r -h ip.txt test/ /tmp/dir1/</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash">prsync 并行传输文件到远端</span></span><br><span class="line"><span class="meta">#</span><span class="bash">传文件,支持远程新建目录,即目录不存在则新建</span></span><br><span class="line">prsync -h ip.txt test.py /tmp/dir2/</span><br><span class="line"><span class="meta">#</span><span class="bash">传目录</span></span><br><span class="line">prsync -r -h ip.txt test/ /tmp/dir3/</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash">pslurp从远程拉取文件到本地,在本地自动创建目录名为远程主机ip的目录,将拉取的文件放在对应主机IP目录下</span></span><br><span class="line"><span class="meta">#</span><span class="bash">格式:pslurp -h ip.txt -L &lt;本地目录&gt;  &lt;远程目录/文件&gt;  &lt;本地重命名&gt;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">拉取文件</span></span><br><span class="line">pslurp -h ip.txt -L /root/ /root/1.jpg picture</span><br><span class="line">ll /root/172.16.1.13/picture</span><br><span class="line">-rw-r--r-- 1 root root 148931 Jan  9 15:41 /root/172.16.1.13/picture</span><br><span class="line"><span class="meta">#</span><span class="bash">拉取目录</span></span><br><span class="line">pslurp -r -h ip.txt -L /root/ /root/test temp</span><br><span class="line">ll -d /root/172.16.1.13/temp/</span><br><span class="line">drwxr-xr-x 2 root root 23 Jan  9 15:49 /root/172.16.1.13/temp/</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash">pnuke:远程批量killall</span></span><br><span class="line">pnuke -h ip.txt nginx</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    
    
    <!-- 文章结束分割线 -->
    <div style="text-align:center;color: #ccc;font-size:14px;">---------------- The End ----------------</div>

    

    

    
    
    <div>
	    <p id="div-border-left-red">
	    <span>
        <b>作者: brooksjay</b><br/>
        <b>联系邮箱: jaypark@smail.nju.edu.cn</b><br/>
	    <b>本文地址: </b><a href="/index.html" title=""></a><br/>
	    <b>本文基于<a target="_blank" title="Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)" href="http://creativecommons.org/licenses/by-sa/4.0/"> 知识共享署名-相同方式共享 4.0 </a>国际许可协议发布</b><br/>
        <b>转载请注明出处, 谢谢！</b>
	    </span>
	    </p>
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      


  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">

    <link itemprop="mainEntityOfPage" href="https://brooksj.com/2019/09/23/%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0-Catalan-Number/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="brooksj">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="在人间漂流">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/23/%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0-Catalan-Number/" itemprop="url">卡特兰数(Catalan Number)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-23T19:18:40+08:00">
                2019-09-23
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Math/" itemprop="url" rel="index">
                    <span itemprop="name">Math</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/09/23/%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0-Catalan-Number/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="https://brooksj.com/2019/09/23/%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0-Catalan-Number/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          
          
             <span id="/2019/09/23/%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0-Catalan-Number/" class="leancloud_visitors" data-flag-title="卡特兰数(Catalan Number)">
               <span class="post-meta-divider"><br/></span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          <span class="post-time">
	        &nbsp; | &nbsp;
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">字数统计:</span>
            <span class="post-count">1.4k(字)</span>
            
          </span>
	  
          <span class="post-time">
	        &nbsp; | &nbsp;
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">阅读时长:</span>
            <span class="post-count">5(分)</span>
          </span>

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>卡特兰数又称卡塔兰数，英文名Catalan number，是组合数学中一个常出现在各种计数问题中的数列。该数在计算机专业中比较重要，有一些具体的应用实例。这篇文章主要分三部分：</p>
<ol>
<li>卡特兰数递归式的含义解释</li>
<li>卡特兰数表达式的证明过程</li>
<li>卡特兰数在计算机中的应用</li>
</ol>
<h2>Catalan Number递归式解释</h2>

<p>假设h(0)=1，h(1)=1，catalan数满足递推式：</p>
<script type="math/tex; mode=display">
h(n) = h(0)*h(n-1) + h(1)*h(n-2) + h(2)*h(n-3) + ... +h(n-1)*h(0) \tag{1.1}</script><p>递归式背后有什么物理含义呢，这里以出栈序列问题进行说明：</p>
<font color='red'>问题描述：</font>一个栈(无穷大)的进栈序列为1，2，3，…，n，有多少个不同的出栈序列?<br>

<font color='green'>含义解释：</font>首先，我们设$h(n)$=序列个数为n的出栈序列种数。（我们假定，最后出栈的元素为k，显然，k取不同值时的情况是相互独立的，也就是求出每种k最后出栈的情况数后可用加法原则，由于k最后出栈，因此，在k入栈之前，比k小的值均出栈，此处情况有$h(k-1)$种，而之后比k大的值入栈，且都在k之前出栈，因此有$h(n-k)$种方式，由于比k小和比k大的值入栈出栈情况是相互独立的，此处可用乘法原则，$h(n-k)*h(k-1)$种，求和便是Catalan递归式。



<h2>Catalan Number表达式证明</h2>

第n个卡特兰数h(n)表达式如下
$$
h(n)=\frac{C_{2n}^{n}}{n+1}=C_{2n}^{n}-C_{2n}^{n-1} \tag{1.2}
$$
具体证明过程如下

<div align="center">
    <img src="/images/catalan1.jpeg">
</div>

<div align="center">
    <img src="/images/catalan2.jpeg">
</div>

<div align="center">
    <img src="/images/catalan3.jpeg">
</div>

为了便于编程实现，需要进一步推导$h(n)$与$h(n-1)$之间的关系

已知$h(n)$，易知
$$
h(n-1)=\frac{C_{2n-2}^{n-1}}{n}
$$
推导$h(n)$的$C_{2n}^{n}$和$h(n-1)$的$C_{2n-2}^{n-1}$之间的关系，由$kC_{n}^{k}=nC_{n-1}^{k-1}$知
$$
\begin{align}
n*C_{2n}^{n}&=2nC_{2n-1}^{n-1} \\
C_{2n}^{n}&=2C_{2n-1}^{n-1} \\
C_{2n}^{n}&=2\frac{(2n-1)C_{2n-2}^{n-1}}{n} \\
C_{2n}^{n}&=2(2n-1)h(n-1) \\
\frac{C_{2n}^{n}}{n+1}&=\frac{2(2n-1)}{n+1}h(n-1) \\
h(n)&=\frac{2(2n-1)}{n+1}h(n-1)
\end{align}
$$
最终得到$h(n)$和$h(n-1)$之间的递归式$h(n)=\frac{2(2n-1)}{n+1}h(n-1)$



<h2>Catalan Number应用实例</h2>

<h3>括号匹配问题</h3>

<font color='red'>问题描述: </font>矩阵连乘 $P=A_1A_2...A_n$，依据乘法结合律，不改变其顺序，只用括号表示成对的乘积，问有几种括号化的方案？<br>

问题转换一下就是n对括号的正确匹配方案，可以做一下<a href="https://leetcode.com/problems/generate-parentheses/" target="_blank" rel="noopener">LeetCode-22</a>

<h3>出栈次序问题</h3>

<font color='red'>问题描述: </font>一个栈(无穷大)的进栈序列为1,2,3,..n,有多少个不同的出栈序列?<br>

出栈问题问题正是卡特兰数递归式$h(n)=h(0)h(n-1)+h(1)h(n-2)+...+h(n-1)h(0)$的由来

<h3>相关应用问题</h3>

1. 有2n个人排成一行进入剧场，入场费5元。其中只有n个人有一张5元钞票，另外n人只有10元钞票，剧院无其它钞票，问有多少种方法使得只要有10元的人买票，售票处就有5元的钞票找零？(将持5元者到达视作将5元入栈，持10元者到达视作使栈中某5元出栈)<br>

2. n个1和n个0组成一个2n位的二进制数，要求从左到右扫描，0的累计数不小于1的累计数，求满足条件的的数。<br>

3. 12个人排成两排，每排必须是从矮到高排列，而且第二排比对应的第一排的人高，问排列方式有多少种？<br>

   我们先把这12个人从低到高排列，然后，选择6个人排在第一排，那么剩下的6个肯定是在第二排。对问题进行转化：用0表示对应的人在第一排，用1表示对应的人在第二排，那么含有6个0，6个1的序列，并且任意前缀中0的个数大于等于1的个数就对应一种方案，转化后的问题就是问题2了。<br>

4. 给定节点组成二叉树的问题：给定n个节点，能构成多少种形状不同的二叉树？<br>

   先取一个点作为顶点，然后左边依次可以取0至n-1个相对应的，右边是n-1到0个，两两配对相乘，就是$h(0)*h(n-1) + h(2)*h(n-2) + ... + h(n-1)h(0)=h(n)$能构成$h(n)$个，因此二叉树问题也可以解释卡特兰数递归式(1.1)式的由来<br>

5. n*n棋盘从左上角(0,0)走到右下角(n,n)而不跨过主对角线的走法？<br>

   要从左上角(0,0)走到右下角(n,n)则必须向下走n步，向右n步，同时为了不跨过主对角线(允许在主对角线上)，则走过的步数中向下走的步数必须大于等于向右走的步数，剖析之后发现这个问题与问题3是等价问题，走法有卡特兰数$h(n)$种。可以做一下下面两题练练手：<br>

   <a href="http://acm.hdu.edu.cn/showproblem.php?pid=2067" target="_blank" rel="noopener">hdoj2067-小兔的棋盘</a><br>

   <a href="https://leetcode.com/problems/unique-paths/" target="_blank" rel="noopener">LeetCode62-Unique Paths</a><br>

6. n个+1和n个-1构成的2n项序列，其部分和总满足：$a_1+a_2+...+a_n>=0$的序列的个数。<br>

   卡特兰数表达式(1.2)式就是以该问题模型为基础推导出来的<br><br>



<font color="green" size=4>参考链接:</font>

<ol>
<li><a href="https://blog.csdn.net/ACdreamers/article/details/7628667" target="_blank" rel="noopener">https://blog.csdn.net/ACdreamers/article/details/7628667</a></li>
</ol>

          
        
      
    </div>
    
    
    
    
    <!-- 文章结束分割线 -->
    <div style="text-align:center;color: #ccc;font-size:14px;">---------------- The End ----------------</div>

    

    

    
    
    <div>
	    <p id="div-border-left-red">
	    <span>
        <b>作者: brooksjay</b><br/>
        <b>联系邮箱: jaypark@smail.nju.edu.cn</b><br/>
	    <b>本文地址: </b><a href="/index.html" title=""></a><br/>
	    <b>本文基于<a target="_blank" title="Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)" href="http://creativecommons.org/licenses/by-sa/4.0/"> 知识共享署名-相同方式共享 4.0 </a>国际许可协议发布</b><br/>
        <b>转载请注明出处, 谢谢！</b>
	    </span>
	    </p>
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      


  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">

    <link itemprop="mainEntityOfPage" href="https://brooksj.com/2019/09/18/linux%E4%B8%8B%E5%AE%89%E8%A3%85nvidia-driver-cuda10-cudnn7-tensorflow1-14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="brooksj">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="在人间漂流">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/18/linux%E4%B8%8B%E5%AE%89%E8%A3%85nvidia-driver-cuda10-cudnn7-tensorflow1-14/" itemprop="url">ubuntu下安装nvidia driver, cuda10, cudnn7, tensorflow1.14</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-18T11:28:12+08:00">
                2019-09-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/Tensorflow/" itemprop="url" rel="index">
                    <span itemprop="name">Tensorflow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/09/18/linux%E4%B8%8B%E5%AE%89%E8%A3%85nvidia-driver-cuda10-cudnn7-tensorflow1-14/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="https://brooksj.com/2019/09/18/linux%E4%B8%8B%E5%AE%89%E8%A3%85nvidia-driver-cuda10-cudnn7-tensorflow1-14/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          
          
             <span id="/2019/09/18/linux%E4%B8%8B%E5%AE%89%E8%A3%85nvidia-driver-cuda10-cudnn7-tensorflow1-14/" class="leancloud_visitors" data-flag-title="ubuntu下安装nvidia driver, cuda10, cudnn7, tensorflow1.14">
               <span class="post-meta-divider"><br/></span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          <span class="post-time">
	        &nbsp; | &nbsp;
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">字数统计:</span>
            <span class="post-count">1.8k(字)</span>
            
          </span>
	  
          <span class="post-time">
	        &nbsp; | &nbsp;
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">阅读时长:</span>
            <span class="post-count">7(分)</span>
          </span>

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="安装nvidia-driver"><a href="#安装nvidia-driver" class="headerlink" title="安装nvidia driver"></a>安装nvidia driver</h2><ul>
<li><p>查看本机显卡</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lspci | grep -i VGA</span><br></pre></td></tr></table></figure>
<p>终端输出显卡名称，现在电脑一般都有集显+独显2块显卡，若都是nvidia公司的，后续如果安装openGL就不会冲突，因为openGL只支持nvidia的显卡，其他公司的会被openGL安装覆盖。我这里是intel集显，所以安装cuda的时候就不能安装openGL.</p>
</li>
<li><p>查看本机nvidia GPU型号</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lspci | grep -i nvidia</span><br></pre></td></tr></table></figure>
<p>到官网<a href="https://www.nvidia.cn/Download/index.aspx?lang=cn#" target="_blank" rel="noopener">official nvidia driver</a>下载对应自己系统版本和GPU型号的driver，cuda和nvidia driver的对应关系可以参考<a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html" target="_blank" rel="noopener">cuda vs nvidia-driver</a>。我这里下载的GeForce GTX 1060，linux 64bit，game ready版本的driver。在真实安装之前还需要禁用系统现有的driver。</p>
</li>
<li><p>禁用nouveau driver</p>
<p>nouveau是ubuntu16.04默认安装的第三方开源驱动，安装cuda会跟nouveau冲突，需要事先禁掉，运行命令<code>lsmod | grep nouveau</code>后需要没有任何输出就代表禁掉了。具体禁用方法如下：</p>
<p>在/etc/modprobe.d中创建文件blacklist-nouveau.conf，在文件中添加以下内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">blacklist nouveau</span><br><span class="line">options nouveau modeset&#x3D;0</span><br></pre></td></tr></table></figure>
<p>命令行下执行<code>sudo update-initramfs –u</code>，然后在执行<code>lsmod | grep nouveau</code>，若无内容输出，则禁用成功，若仍有内容输出，请检查操作，并重复上述操作。</p>
</li>
<li><p>卸载已有nvidia driver</p>
<p>可能本机上已安装过nvidia driver，但是安装更高版本的cuda需要安装更高版本的nvidia driver，查看系统是否已安装的nvidia driver</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg --list | grep nvidia-*  # dpkg安装的</span><br><span class="line">sudo apt list | grep nvidia-*  # apt安装的</span><br></pre></td></tr></table></figure>
<p>如果包含nvidia-*开头的一系列文件则说明系统已安装过nvidia driver，执行以下命令下载已有驱动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg purge nvidia-* # dpkg或.run安装的</span><br><span class="line">sudo /usr/bin/nvidia-uninstall # cuda打包安装的或者dpkg或.run安装的(建议)</span><br></pre></td></tr></table></figure>
</li>
<li><p>正式安装nvidia driver</p>
<p><code>ctrl+alt+f1</code>进入文字界面(<code>ctrl+alt+f7</code>回到图形桌面)，执行以下命令关闭图形界面</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service lightdm stop</span><br></pre></td></tr></table></figure>
<p>进入到nvidia driver runfile所在目录执行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod a+x NVIDIA-*.run</span><br><span class="line">./NVIDIA-*.run  --no-opengl-files  # 务必加上--no-opengl-files，否则安装过后将处于登录界面而无法进入桌面</span><br></pre></td></tr></table></figure>
<p>参数解释：</p>
<ul>
<li><code>–no-opengl-files</code>：表示只安装驱动文件，不安装OpenGL文件。这个参数不可省略，否则会导致登陆界面死循环，英语一般称为”login loop”或者”stuck in login”。</li>
<li><code>–no-x-check</code>：表示安装驱动时不检查X服务，非必需。</li>
<li><code>–no-nouveau-check</code>：表示安装驱动时不检查nouveau，非必需。</li>
<li><code>-Z, --disable-nouveau</code>：禁用nouveau。此参数非必需，因为之前已经手动禁用了nouveau(建议手动禁用)。</li>
</ul>
<ul>
<li><code>-A</code>：查看更多高级选项。</li>
</ul>
<blockquote>
<p>必选参数解释：因为NVIDIA的驱动默认会安装OpenGL，而Ubuntu的内核本身也有OpenGL、且与GUI显示息息相关，一旦NVIDIA的驱动覆写了OpenGL，在GUI需要动态链接OpenGL库的时候就引起问题。提示安装基本上都是accept，yes，当提示你nvidia-xconfig时，就视自己的电脑情况而定，如果电脑是双显卡（双独显、集显和独显）就选择不安装，如果只有一个显卡就选择安装。我的电脑是intel集显+nvidai独显，所以拒绝安装nvidia-xconfig。</p>
</blockquote>
</li>
</ul>
<h2 id="安装cuda10"><a href="#安装cuda10" class="headerlink" title="安装cuda10"></a>安装cuda10</h2><p>在官网<a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">nvidia cuda downloads</a>下载对应版本的cuda，然后根据自己硬件和系统选择合适的cuda进行安装，我这里下载的cuda10，下载选项如下</p>
<div align="center">
    <img src="/images/cuda1.png">
</div>

<p>下载好之后进入到所在目录进行安装</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod a+x cuda-*.run</span><br><span class="line">./cuda-*.run  --no-opengl --tmpdir=/usr/local/tmp</span><br></pre></td></tr></table></figure>
<p>安装过程中要借用/tmp目录，如果/tmp目录空间不足可以用—tmpdir指定一个tmp目录如上所示。安装过程中会询问你安装各种各样的东西，除了cuda toolkit，其它的都不需要安装，安装路径自己确定也可以保持默认，确认建立软连接到/usr/local/cuda。如果有补丁patch，则在安装完主模块之后再安装patch，方法一致。</p>
<h2 id="安装cudnn7"><a href="#安装cudnn7" class="headerlink" title="安装cudnn7"></a>安装cudnn7</h2><p>在官网<a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">nvidia cudnn downloads</a>下载对应cuda版本的cudnn，cudnn下载要求必须登录账户才可以，我这里下载最新的cuDNN v7.6.3 for cuda 10 .0，下载好之后解压，然后将其库文件copy到cuda中</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cp	CUDNN_HOME/include/cudnn.h	CUDA_HOME/include</span><br><span class="line">cp	CUDNN_HOME/lib64/libcudnn*	CUDA_HOME/lib64/</span><br><span class="line">ln	-sf  CUDA_HOME/lib64/lincudnn.so.7.6.3	CUDA_HOME/lib64/libcudnn.so.7</span><br><span class="line">ln	-sf  CUDA_HOME/lib64/libcudnn.so.7  CUDA_HOME/lib64/libcudnn.so</span><br><span class="line">sudo mkdir /etc/ld.so.conf.d/nvidia.conf</span><br><span class="line">sudo cat CUDA_HOME/lib64</span><br><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure>
<p>ldconfig要求.so文件是软链接，所以必须ln -sf之后再执行ldconfig上面命令中的CUDNN_HOME和CUDA_HOME分别是cudnn和cuda安装目录，安装好之后配置环境~/.bashrc</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CUDA</span></span><br><span class="line"><span class="built_in">export</span> CUDA_HOME=/home/liujian/cuda-9.0</span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$CUDA_HOME</span>/lib64:<span class="variable">$CUDA_HOME</span>/extras/CUPTI/lib64:<span class="variable">$LD_LIBRARY_PATH</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$CUDA_HOME</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure>
<p>source ~/.bashrc之后，执行<code>nvcc -V</code>查看cuda版本，测试cuda是否已可用</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd CUDA_HOME/samples/1_Utilities/deviceQuery</span><br><span class="line">make</span><br><span class="line">./deviceQuery</span><br></pre></td></tr></table></figure>
<p>如果最后显示pass则表明cuda安装成功，否则不可用</p>
<h2 id="源码编译安装tensorflow1-14"><a href="#源码编译安装tensorflow1-14" class="headerlink" title="源码编译安装tensorflow1.14"></a>源码编译安装tensorflow1.14</h2><p>安装tensorflow之前查看cuda和cudnn版本号以安装相匹配版本的tensorflow</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看cuda版本</span></span><br><span class="line">cat CUDA_HOME/version.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看cudnn版本</span></span><br><span class="line">cat cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2</span><br></pre></td></tr></table></figure>
<p>git clone tensorflow的官方仓库</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/tensorflow/tensorflow.git</span><br></pre></td></tr></table></figure>
<p>基于tag(版本)创建分支</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git tag # 查看分支</span><br><span class="line">git branch r1.14.0  v1.14.0  # 基于tagv1.14.0建立r1.14.0分之</span><br><span class="line">git checkout r1.14.0</span><br></pre></td></tr></table></figure>
<p>使用python3.6安装，tensorflow的源码编译安装还需要使用google的一款编译器bazel，具体安装教程以及tensoflow版本与cuda,cudnn,bazel的版本匹配请参考官网的<a href="https://tensorflow.google.cn/install/source" target="_blank" rel="noopener">tensorflow install</a>.</p>
<font color='red' size=5>Caution:</font>

<p>这里记录一下我安装过程中出现的几个问题及经验，在正式bazel编译之前执行<code>./configure</code>进行配置，配置过程中建议使用gcc而不是clang，这样编译过程中不容易出错，然后配置过程中可能会报以下错误</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File "third_party/gpus/find_cuda_config.py", line 463, in &lt;module&gt;</span><br><span class="line">    main()</span><br><span class="line">  File "third_party/gpus/find_cuda_config.py", line 455, in main</span><br><span class="line">    for key, value in sorted(find_cuda_config().items()):</span><br><span class="line">  File "third_party/gpus/find_cuda_config.py", line 418, in find_cuda_config</span><br><span class="line">    _get_default_cuda_paths(cuda_version))</span><br><span class="line">  File "third_party/gpus/find_cuda_config.py", line 159, in _get_default_cuda_paths</span><br><span class="line">    ] + _get_ld_config_paths()</span><br><span class="line">  File "third_party/gpus/find_cuda_config.py", line 139, in _get_ld_config_paths</span><br><span class="line">    match = pattern.match(line.decode("ascii"))</span><br><span class="line">UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 27: ordinal not in range(128)</span><br><span class="line">Asking for detailed CUDA configuration...</span><br></pre></td></tr></table></figure>
<p>这个问题可以参考<a href="https://github.com/tensorflow/tensorflow/pull/28209" target="_blank" rel="noopener">github issue: find_cuda</a>进行解决，具体做法：在third_party/gpus/find_cuda_config.py文件中找到<font color='red'>match = pattern.match(line.decode(“ascii”))</font>，并将其修改为<font color='green'>match = pattern.match(line.decode(sys.stdin.encoding))</font>即可，重新执行<code>./configure</code>就不会再报错了。</p>
<font color='magenta' size=4>写在最后：</font>

<p>google提供了tensorflow多标签的docker镜像，使用docker容器安装使用tensorflow是最便捷且安全的，用户可以在tensorflow多版本之间自由切换，在服务器上使用也不用再受权限问题困扰了，而且最新的docker 19.03已经原生支持容器使用物理机上的gpu了，不再需要安装nvidia-docker来支持gpu使用了，用户只需在物理机上安装nvidia驱动，其它的都有镜像提供，喜大普奔。docker安装可以参考我之前写的一篇博客<a href="https://www.cnblogs.com/brooksj/p/11456329.html" target="_blank" rel="noopener">Linux下docker安装教程</a>，至于docker下使用tensorflow镜像的教程可以参考tensorflow官方教程<a href="https://tensorflow.google.cn/install/docker" target="_blank" rel="noopener">Docker</a>。</p>

          
        
      
    </div>
    
    
    
    
    <!-- 文章结束分割线 -->
    <div style="text-align:center;color: #ccc;font-size:14px;">---------------- The End ----------------</div>

    

    

    
    
    <div>
	    <p id="div-border-left-red">
	    <span>
        <b>作者: brooksjay</b><br/>
        <b>联系邮箱: jaypark@smail.nju.edu.cn</b><br/>
	    <b>本文地址: </b><a href="/index.html" title=""></a><br/>
	    <b>本文基于<a target="_blank" title="Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)" href="http://creativecommons.org/licenses/by-sa/4.0/"> 知识共享署名-相同方式共享 4.0 </a>国际许可协议发布</b><br/>
        <b>转载请注明出处, 谢谢！</b>
	    </span>
	    </p>
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      


  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">

    <link itemprop="mainEntityOfPage" href="https://brooksj.com/2019/09/16/Ubuntu%E7%BE%8E%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="brooksj">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="在人间漂流">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/16/Ubuntu%E7%BE%8E%E5%8C%96/" itemprop="url">Ubuntu18.04美化</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-16T16:18:59+08:00">
                2019-09-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/09/16/Ubuntu%E7%BE%8E%E5%8C%96/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count fb-comments-count" data-href="https://brooksj.com/2019/09/16/Ubuntu%E7%BE%8E%E5%8C%96/" itemprop="commentCount">0</span> comments
                </a>
              </span>
            
          

          
          
             <span id="/2019/09/16/Ubuntu%E7%BE%8E%E5%8C%96/" class="leancloud_visitors" data-flag-title="Ubuntu18.04美化">
               <span class="post-meta-divider"><br/></span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          <span class="post-time">
	        &nbsp; | &nbsp;
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">字数统计:</span>
            <span class="post-count">1.7k(字)</span>
            
          </span>
	  
          <span class="post-time">
	        &nbsp; | &nbsp;
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">阅读时长:</span>
            <span class="post-count">6(分)</span>
          </span>

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>ubuntu桌面比较简陋，这就驱使很多人想DIY自己的一套桌面主题，如果是从零开始配置和美化ubuntu18.04，可以建议先参考<a href="https://blog.csdn.net/ice__snow/article/details/80152068" target="_blank" rel="noopener">Ubuntu 18.04配置及美化</a></p>
<h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h2><blockquote>
<p>工欲善其事，必先利其器</p>
</blockquote>
<p>要对ubuntu进行美化就必定要用到神器gnome-tweak-tool，这是 Gnome 官方发布的一款 Gnome 调节软件, 借助这款软件, 我们可以更好地管理主题, 扩展, 字体 以及系统行为等设置项。安装方式很简单，命令行下执行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install gnome-tweak-tool</span><br></pre></td></tr></table></figure>
<p>安装tweak好之后打开界面如下</p>
<div align="center">
    <img src="/images/tweak.png">
</div>

<h2 id="gnome扩展"><a href="#gnome扩展" class="headerlink" title="gnome扩展"></a>gnome扩展</h2><p><a href="https://extensions.gnome.org/" target="_blank" rel="noopener">Gnome Shell Extensions</a> 是 Gnome 的一系列插件, 类似 Chrome 的插件, 可以起到系统增强的作用。借助chrome的插件可以方便的访问gnome扩展网站并实现一键添加或删除gnome扩展程序，具体安装过程两步</p>
<ul>
<li>安装 Chrome 扩展程序 <a href="https://chrome.google.com/webstore/detail/gnome-shell-integration/gphhapmejobijbbhgpjhcjognlahblep" target="_blank" rel="noopener">GNOME Shell integration</a></li>
<li>安装 主机连接器 <code>sudo apt install chrome-gnome-shell</code></li>
</ul>
<p>接下来我们就可以在网站 <a href="https://extensions.gnome.org/" target="_blank" rel="noopener">GNOME Shell Extensions</a> 安装 gnome 扩展了。</p>
<p>通过搜索找到自己心仪的扩展程序，点击进入详情页面，切换详情页面的“OFF”按钮即可安装对应扩展，如下图红圈标示</p>
<div align="center">
    <img src="/images/gnome-shell-extension.png">
</div>

<p>点击红叉即可卸载该扩展程序。有了tweak和gnome shell extension之后就可以开始DIY自己的ubuntu桌面了。</p>
<h2 id="安装gnome扩展"><a href="#安装gnome扩展" class="headerlink" title="安装gnome扩展"></a>安装gnome扩展</h2><p>先安装一些好用的扩展程序以帮助我们提高工作效率，可参考<a href="https://www.jianshu.com/p/5bd14cbf7186" target="_blank" rel="noopener">简书：Ubuntu 18.10 美化</a></p>
<ul>
<li>dash to dock 优化 Ubuntu 默认的 dock</li>
<li>User Themes 自定义 shell 主题</li>
<li>Coverflow Alt-Tab 优化 Ubuntu 默认窗口切换动作</li>
<li>Gnome Global Application Menu 将当前程序的菜单项提取到状态栏</li>
<li>NetSpeed 显示网速插件</li>
<li>Clipboard Indicator 提供剪切板历史记录功能</li>
<li>Drop Down Terminal 可以从屏幕上快速弹出一个终端</li>
<li>Recent Items 快速打开最近打开过的文件</li>
<li>Places Status Indicator 利用下拉菜单快速打开驱动器上的常用位置</li>
<li>Dynamic Top Bar 动态调整状态栏透明度</li>
<li>Hide top bar 隐藏顶栏, 可以设置为鼠标靠近屏幕上边沿时显示顶栏</li>
<li>Top Panel Workspace Scroll快速切换工作区</li>
<li>Gravatar 把你的 Ubuntu 用户头像设置成你的 Gravatar 头像.</li>
<li>TopIcons Plus 将传统托盘图标移动到顶部面板 (Wine 程序救星)</li>
</ul>
<p>按下 Alt + F2,输入 r，回车重启 gnome。</p>
<h2 id="安装theme和icon"><a href="#安装theme和icon" class="headerlink" title="安装theme和icon"></a>安装theme和icon</h2><p>有了便捷的扩展程序以后，再搭配一个让人赏心悦目的主题岂不美哉。maxOS的桌面风格很受程序猿的喜欢，所以先安利一款macOS主题桌面<a href="https://www.gnome-look.org/p/1013714/" target="_blank" rel="noopener">MCHigh Sierra</a>，具体的制作过程可参考<a href="https://ywnz.com/linuxmh/2105.html" target="_blank" rel="noopener">Ubuntu18.04主题更换为Mac OS high Sierra美化教程</a>，按照里面的教程一步一步来即可制作出属于自己的macOS主题桌面。更多的MacOS主题安装教程可以参考<a href="https://www.cnblogs.com/feipeng8848/p/8970556.html" target="_blank" rel="noopener">给Ubuntu18.04(18.10)安装mac os主题</a></p>
<blockquote>
<font color='red'>Caution:</font>

<p>Sierra的原始资源地址<a href="https://www.gnome-look.org/p/1013714/" target="_blank" rel="noopener">McHigh Sierra</a>仅提供了gnome应用主题，没有提供图标风格icon，想要获取对应主题的icon可从<a href="https://github.com/vbay/CSDN-CODE/tree/master/Ubuntu18.04-tutorials-themes" target="_blank" rel="noopener">Ubuntu18.04-tutorials-themes</a>获取</p>
</blockquote>
<p>除了macOS主题之外还有很多其它好看的主题，可以根据个人喜好在<a href="https://www.gnome-look.org/" target="_blank" rel="noopener">gnome-look</a>中进行查找。比如我个人喜欢的一套主题是<a href="https://github.com/vinceliuice/vimix-gtk-themes" target="_blank" rel="noopener">Vimx-beryl Theme</a>及其配套icon<a href="https://github.com/vinceliuice/vimix-icon-theme" target="_blank" rel="noopener">Vimx-beryl Icon</a>，这套主题的具体安装教程可参考<a href="https://zhuanlan.zhihu.com/p/37314255" target="_blank" rel="noopener">知乎：Ubuntu 18.04 LTS 安装、美化</a></p>
<blockquote>
<font color='red'>Caution:</font>

<p>下载的theme可以放在系统themes目录/usr/share/themes下，也可以放在用户目录~/.themes下。而下载的icon可以放在系统icons目录/usr/share/icons下，也可以放在用户目录~/.icons下</p>
</blockquote>
<h2 id="安装gnome-shell"><a href="#安装gnome-shell" class="headerlink" title="安装gnome shell"></a>安装gnome shell</h2><p><a href="https://www.gnome-look.org/p/1013030/" target="_blank" rel="noopener">Flat Remix</a>个人挺喜欢的一款gnome shell风格，可以通过添加源在命令行下安装</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository ppa:daniruiz/flat-remix</span><br><span class="line">sudo apt-get install flat-remix</span><br><span class="line">sudo apt-get install flat-remix-gnome</span><br><span class="line">sudo apt install gnome-shell-extensions</span><br></pre></td></tr></table></figure>
<p>然后重新打开tweak，在扩展一栏中将User themes打开之后即可切换gnome shell的风格了</p>
<h2 id="安装zsh"><a href="#安装zsh" class="headerlink" title="安装zsh"></a>安装zsh</h2><p>zsh是mac默认的shell，而ubuntu的默认shell是bash。相比bash，zsh配合<a href="https://github.com/robbyrussell/oh-my-zsh/wiki/themes" target="_blank" rel="noopener">oh-my-zsh</a>拥有更丰富的主题，使得命令行更为美观。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install zsh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 切换到zsh</span></span><br><span class="line">chsh -s /bin/zsh</span><br></pre></td></tr></table></figure>
<p>重新登录shell即可转换到zsh，接下来在用户主目录下安装<code>oh-my-zsh</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -c "$(wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)"</span><br></pre></td></tr></table></figure>
<p>接着安装插件<code>highlight</code>，高亮语法</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~/.oh-my-zsh/custom/plugins</span><br><span class="line">git clone git://github.com/zsh-users/zsh-syntax-highlighting.git</span><br></pre></td></tr></table></figure>
<p>在<code>Oh-my-zsh</code>的配置文件中<code>~/.zshrc</code>中添加插件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 括号中的插件名以空格分隔</span></span><br><span class="line">plugins=( [plugins...] zsh-syntax-highlighting)</span><br></pre></td></tr></table></figure>
<p>设置zsh主题</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 个人觉得比较好看的zsh主题有: robbyrussell(default), agnoster, bira</span></span><br><span class="line">ZSH_THEME="robbyrussell"</span><br></pre></td></tr></table></figure>
<blockquote>
<font color='red'>Caution:</font>

<p>其中<a href="https://github.com/agnoster/agnoster-zsh-theme" target="_blank" rel="noopener">agnoster</a>主题比较像之前在bash下使用的powerline，由于包含特殊的字体符号，需要安装额外的字体 <a href="https://github.com/Lokaltog/powerline-fonts" target="_blank" rel="noopener">Powerline-patched font</a>才能支持主题正常显示，ubuntu下可直接使用apt安装</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install fonts-powerline</span><br><span class="line">fc-cache -vf /usr/share/fonts/  #更新系统的字体缓存</span><br></pre></td></tr></table></figure>
</blockquote>
<p>zsh主题定制可以参考<a href="https://www.jianshu.com/p/bf488bf22cba" target="_blank" rel="noopener">oh-my-zsh终端用户名设置（PS1）</a>，zsh的一些主题例如agnoster会自动添加命令行头名user@host，如果觉得这种形式使得命令行看起来很臃肿，可以在~/.zshrc中可以设置DEFAULT_USER来避免</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DEFAULT_USER=brooksj</span><br></pre></td></tr></table></figure>
<p>避免命令行头名臃肿还可以通过prompt_context() {}来设置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 隐藏用户名和主机名</span></span><br><span class="line">prompt_context() &#123;&#125;                                                           </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 只保留用户名，隐藏主机名</span></span><br><span class="line">prompt_context() &#123;</span><br><span class="line">  if [[ "$USER" != "$DEFAULT_USER" || -n "$SSH_CLIENT" ]]; then</span><br><span class="line">    prompt_segment black default "%(!.%&#123;%F&#123;yellow&#125;%&#125;.)$USER"</span><br><span class="line">  fi  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>个人倾向于只保留用户名，最后使配置生效</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.zshrc</span><br></pre></td></tr></table></figure>
<h2 id="安装background"><a href="#安装background" class="headerlink" title="安装background"></a>安装background</h2><p>backgound的图片可从壁纸网站<a href="[https://wallhaven.cc](https://wallhaven.cc/">wallhaven</a>)下载，然后放到/usr/share/background中，右击桌面切换背景即可</p>
<h2 id="个人配置展示"><a href="#个人配置展示" class="headerlink" title="个人配置展示"></a>个人配置展示</h2><p>个人所使用的应用程序、光标、图标、shell在tweak中配置如下图所示</p>
<div align="center">
    <img src="/images/tweakconfig.png">
</div>

<p>个人用到的扩展(gnome shell extensions)为</p>
<ol>
<li>Clipboard Indicator 提供剪切板历史记录功能</li>
<li>dash to dock 优化 Ubuntu 默认的 dock</li>
<li>User Themes 自定义 shell 主题</li>
<li>Coverflow Alt-Tab 优化 Ubuntu 默认窗口切换动作</li>
<li>Drop Down Terminal 可以从屏幕上快速弹出一个终端</li>
<li>Gravatar 把你的 Ubuntu 用户头像设置成你的 Gravatar 账户头像.</li>
<li>NetSpeed 显示网速插件</li>
<li>Recent Items 快速打开最近打开过的文件</li>
<li>Places Status Indicator 利用下拉菜单快速打开驱动器上的常用位置</li>
<li>Top Panel Workspace Scroll快速切换工作区</li>
</ol>
<font size=4 color='green'>参考链接：</font>

<ol>
<li><a href="https://blog.csdn.net/ice__snow/article/details/80152068" target="_blank" rel="noopener">Ubuntu 18.04配置及美化</a>—-从零开始配置的建议入手</li>
<li><a href="https://zhuanlan.zhihu.com/p/37314255" target="_blank" rel="noopener">Ubuntu 18.04 LTS 安装、美化</a></li>
<li><a href="https://ywnz.com/linuxmh/2105.html" target="_blank" rel="noopener">Ubuntu18.04主题更换为Mac OS high Sierra美化教程</a></li>
<li><a href="https://www.cnblogs.com/feipeng8848/p/8970556.html" target="_blank" rel="noopener">给Ubuntu18.04安装mac os主题</a> </li>
<li><a href="https://www.jianshu.com/p/5bd14cbf7186" target="_blank" rel="noopener">Ubuntu 18.10 美化</a></li>
</ol>

          
        
      
    </div>
    
    
    
    
    <!-- 文章结束分割线 -->
    <div style="text-align:center;color: #ccc;font-size:14px;">---------------- The End ----------------</div>

    

    

    
    
    <div>
	    <p id="div-border-left-red">
	    <span>
        <b>作者: brooksjay</b><br/>
        <b>联系邮箱: jaypark@smail.nju.edu.cn</b><br/>
	    <b>本文地址: </b><a href="/index.html" title=""></a><br/>
	    <b>本文基于<a target="_blank" title="Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)" href="http://creativecommons.org/licenses/by-sa/4.0/"> 知识共享署名-相同方式共享 4.0 </a>国际许可协议发布</b><br/>
        <b>转载请注明出处, 谢谢！</b>
	    </span>
	    </p>
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/">&lt;i class&#x3D;&quot;fa fa-angle-right&quot;&gt;&lt;&#x2F;i&gt;</a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="brooksj" />
            
              <p class="site-author-name" itemprop="name">brooksj</p>
              <p class="site-description motion-element" itemprop="description">我听说虫洞可以穿梭时空</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">31</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">40</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/tracy-talent" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:jaypark@smail.nju.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.cnblogs.com/brooksj" title="博客旧址" target="_blank">博客旧址</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://kexue.fm" title="苏剑林's blog" target="_blank">苏剑林's blog</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://karpathy.github.io" title="Karpathy's Blog" target="_blank">Karpathy's Blog</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://ruder.io" title="Ruder's Blog" target="_blank">Ruder's Blog</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://medium.com/@makcedward" title="Edward Ma's Blog" target="_blank">Edward Ma's Blog</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://blog.miskcoo.com" title="Miskcoo's Blog" target="_blank">Miskcoo's Blog</a>
                  </li>
                
              </ul>
            </div>
          

          <div id="days"></div>
    <script>
    function show_date_time(){
        window.setTimeout("show_date_time()", 1000);
        BirthDay=new Date("04/20/2019 15:00:00");
        today=new Date();
        timeold=(today.getTime()-BirthDay.getTime());
        sectimeold=timeold/1000
        secondsold=Math.floor(sectimeold);
        msPerDay=24*60*60*1000
        e_daysold=timeold/msPerDay
        daysold=Math.floor(e_daysold);
        e_hrsold=(e_daysold-daysold)*24;
        hrsold=setzero(Math.floor(e_hrsold));
        e_minsold=(e_hrsold-hrsold)*60;
        minsold=setzero(Math.floor((e_hrsold-hrsold)*60));
        seconds=setzero(Math.floor((e_minsold-minsold)*60));
        document.getElementById('days').innerHTML="已运行"+daysold+"天"+hrsold+"小时"+minsold+"分"+seconds+"秒";
    }
function setzero(i){
    if (i<10)
    {i="0" + i};
    return i;
}
show_date_time();
</script>

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">brooksj</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("AJCfamp9AdxK89MonULcgI68-gzGzoHsz", "sVrhDpWMN5wNlfAPwgDHdxPL");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
